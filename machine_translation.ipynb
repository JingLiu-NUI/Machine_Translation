{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "BYQt0nUEbAgN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation - Assignment 2\n",
        "\n",
        "In this task you will develop a neural machine translation (NMT) system to translate text from one language to another. For this, you wil need to chose the data to train the models, perform data processing and train a sequence2sequence neural model.\n"
      ]
    },
    {
      "metadata": {
        "id": "d33hukl0w-Nh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 1- Data Collection and Preprocessing \n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "uAFJXKQaeN-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Task 1  (5 marks)**\n",
        "\n",
        "---\n",
        "\n",
        "There are few datasets to train an NMT system available from Tatoeba Project (http://www.manythings.org/anki/) or OPUS project (http://opus.nlpl.eu/).\n",
        "\n",
        "*  Download a langauge pair (preferably European language) and **extract** the file(s) and upload it to colab\n",
        "*  Create a list of lines by splitting the text file at every occurance of '\\n'\n",
        "*  Print number of sentences\n",
        "*  Limit the amount of senteces to 10,000 lines (but more than 5,000 lines)\n",
        "*  Split the data into train and test [You can split validation set here or while training use kerase validation_split option]\n",
        "*  Print 100th sentence in original script[ not unicode] for source and target language\n"
      ]
    },
    {
      "metadata": {
        "id": "9U9BaH3ozUGf",
        "colab_type": "code",
        "outputId": "db988946-4fc4-4c07-d88f-ca64800e7d8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#your code goes here\n",
        "import numpy as np\n",
        "import keras, tensorflow\n",
        "import gensim\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filename=\"/content/drive/My Drive/Colab Notebooks/fra.txt\"\n",
        "with open(filename , 'r') as dataset:\n",
        "  lines=dataset.read().split('\\n')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7NkJaeq8zg5H",
        "colab_type": "code",
        "outputId": "ca99fcbe-4807-48f5-ab18-1f430d62fdb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(lines))\n",
        "print(lines[100])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "167131\n",
            "Come in.\tEntrez !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x63IEWjUxkJj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Task 2 (5 marks)** \n",
        "\n",
        "---\n",
        "\n",
        "* Add '\\t' to denote begining of sentence and '\\n'  or '<eos\\>' to denote end of the sentence to the each target line.\n",
        "* Preprocess (word tokenisation, lowecasing) the text."
      ]
    },
    {
      "metadata": {
        "id": "pmNr8gnzfCeS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your code goes here\n",
        "import nltk\n",
        "#nltk.download('all')\n",
        "# set the sample number is 8000\n",
        "sampes_num=8000\n",
        "input_texts=[]\n",
        "target_texts=[]\n",
        "source_tokenize=[]\n",
        "target_tokenize=[]\n",
        "Source_vocabulary=set()\n",
        "target_vocabulary=set()\n",
        "#if the length of samples < 8000, get all text sentence, otherwise \n",
        "#get 8000 sentence\n",
        "samples=lines[0:min(8000,len(lines)-1)]\n",
        "for line in samples:\n",
        "    source_data, target_data=line.split('\\t')\n",
        "#     target_data='\\t'+target_data+'\\n'\n",
        "    #tokenize get all words in the sentence\n",
        "    source_tokenize=nltk.word_tokenize(source_data)\n",
        "    target_tokenize=nltk.word_tokenize(target_data)\n",
        "    #add \\t and \\n on the head and end\n",
        "    target_tokenize.insert(0,'\\t')\n",
        "    target_tokenize.insert(len(target_tokenize),'\\n')\n",
        "    #get the vocabulary for source and target\n",
        "    for i in source_tokenize:\n",
        "       Source_vocabulary.add(i.lower())\n",
        "    for i in target_tokenize:\n",
        "      target_vocabulary.add(i.lower())       \n",
        "    #append token to input text\n",
        "    input_texts.append(source_tokenize)\n",
        "    #append token to target text\n",
        "    target_texts.append(target_tokenize)\n",
        "#the max encoder and decoder sequence length is get the max length in the total corpus \n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])  \n",
        "#get the encoder and decoder vocabulary length\n",
        "num_encoder_tokens=len(Source_vocabulary)\n",
        "num_decoder_tokens=len(target_vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x4-mCmoMxAd0",
        "colab_type": "code",
        "outputId": "87ad9e31-a883-4214-ce47-4b6ac4e7f7d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique source language tokens:', num_encoder_tokens)\n",
        "print('Number of unique target language tokens:', num_decoder_tokens)\n",
        "print('Max sequence length of source language:', max_encoder_seq_length)\n",
        "print('Max sequence length of target language:', max_decoder_seq_length)\n",
        "print(\"Source Vocabulary\",Source_vocabulary)\n",
        "print(\"Target Vocabulary\",target_vocabulary)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 8000\n",
            "Number of unique source language tokens: 1833\n",
            "Number of unique target language tokens: 3948\n",
            "Max sequence length of source language: 6\n",
            "Max sequence length of target language: 13\n",
            "Source Vocabulary {'short', 'warmer', 'off-duty', 'manage', 'trains', 'powerless', 'mishap', 'baked', 'six', 'exercised', 'bought', 'afternoon', 'were', 'door', 'helped', 'serious', 'creepy', 'lock', 'file', 'cranky', 'where', 'bounce', 'managing', 'dislike', 'mondays', 'maid', 'despair', 'hurry', 'lift', 'deaf', 'fear', 'vote', 'judge', 'finished', 'candy', 'disagreed', 'meet', 'cake', 'worked', 'braces', 'ached', 'seriously', 'kissed', 'goes', 'table', 'whining', 'ramble', 'adults', 'ok', \"n't\", 'was', 'drank', 'ashamed', 'outraged', 'offer', 'boats', 'fish', 'die', 'cash', 'go', 'weighed', 'remember', 'scary', 'gear', 'cop', 'pooped', 'rush', 'birds', 'hint', 'painted', 'date', 'guess', 'feed', 'recycle', 'laugh', 'trips', 'slow', 'dj', 'men', 'say', 'outgoing', 'medic', 'smart', \"'ve\", 'laughed', 'when', 'starving', 'unhappy', 'satisfied', 'crashed', 'spoon', 'flies', 'crew', 'sense', 'heap', 'beg', 'enough', 'stealing', 'reliable', 'nail', 'plane', 'camels', 'soup', 'thrilling', 'sorry', 'frantic', 'arabic', 'coming', 'realize', 'at', 'eight', 'moved', 'whose', 'am', 'mary', 'says', 'keys', 'sleepy', 'bigot', 'started', 'sweep', 'smell', 'even', 'overslept', 'attack', 'shout', 'bedtime', 'psychic', 'nature', 'care', 'blinked', 'doll', 'keeps', 'finally', 'terrified', 'crushed', 'ham', 'guys', 'dreams', 'beer', 'slapped', 'easygoing', 'unlocked', 'watch', 'quick', 'suit', 'hungarian', 'tools', 'room', 'hungover', 'get', 'gone', 'well', 'feet', 'what', 'jesus', 'friendly', 'wide', 'dream', 'stole', 'break', 'ring', 'sequel', 'failed', 'okay', 'an', 'cracked', 'dogs', 'bad', 'empty', 'cows', 'early', 'walks', 'lovely', 'agrees', 'giddy', 'object', 'off', 'law', 'need', 'lay', 'handled', 'slob', 'violin', 'ok.', 'vanished', 'park', 'more', 'loss', 'dreaming', 'forward', 'wig', 'said', 'guarantee', 'straighten', 'far', 'hurt', 'seafood', 'tough', 'correct', 'wonderful', 'addicted', 'snore', 'closely', 'b', 'right', 'litter', 'want', 'admire', 'tie', 'sleeping', 'stroke', 'jumped', 'evil', 'has', 'spring', 'stuck', 'our', 'bankrupt', 'knits', 'fined', 'explain', 'harm', 'grass', 'merciful', 'with', 'study', 'unarmed', 'insisted', 'snores', 'canned', 'saw', 'hole', 'afraid', 'dancing', 'while', 'sent', '50', 'twice', 'honored', 'loud', 'poker', 'ca', 'british', 'ahead', 'bleeding', 'allow', 'cuff', 'instinct', 'wolf', 'different', 'tricked', 'cold', 'notes', 'another', 'worth', 'doomed', 'i', 'sun', 'it', 'boy', 'waiting', 'after', 'wax', 'evening', 'insane', 'nerve', 'took', 'mom', 'hold', 'seem', 'terrible', 'answer', 'adopted', 'whiff', 'contributed', 'finish', 'head', 'positive', 'week', 'clue', 'chess', 'beat', 'urge', '?', 'canadian', 'wanted', 'demented', 'serve', 'lied', 'bluffing', 'kind', 'funny', 'waited', 'they', 'long', 'unlucky', 'promise', 'music', 'danger', 'outside', 'hear', 'learn', 'drugged', 'coin', 'never', 'control', 'hello', 'beautiful', 'famous', 'furious', 'thud', 'caught', 'shy', 'car', 'few', 'thought', 'chance', 'meat', 'stories', 'contented', 'stink', 'jealous', 'glue', 'nose', 'neutral', 'kiss', 'attend', 'freaked', 'bacon', 'tempt', 'gave', 'listened', 'tried', 'end', 'loves', 'anyway', 'water', 'second', 'mama', 'woke', 'real', 'map', 'sang', 'tycoon', 'helping', 'surrender', 'honor', 'human', 'swiss', 'time', 'puppy', 'asked', 'met', 'forgot', 'else', 'cancel', 'bother', 'stoned', 'trusts', 'cloudy', 'drink', 'wasted', 'wait', 'pigs', 'punctual', 'ready', 'cheated', 'school', 'walking', 'dislikes', 'ax', 'runs', 'burned', 'nuts', 'did', 'talks', 'anxious', 'ours', 'these', 'women', 'fighting', 'north', 'seat', 'married', 'voices', 'selfish', 'knew', 'singing', \"'ll\", 'waste', 'bus', 'hideous', 'fantastic', 'dying', 'looked', 'hat', 'scream', 'gets', 'stop', 'icky', 'matter', 'caviar', 'plumber', 'content', 'exhausted', 'sing', 'bore', 'nerd', 'worn', 'young', 'sharp', 'class', 'travel', 'timid', 'duty', 'cover', 'fat', 'shall', 'bomb', 'idiot', 'perfect', 'sundays', '19', 'left', 'flowers', 'memorize', 'favor', 'dog', 'yours', 'till', 'eyes', 'details', 'heel', 'suicide', 'prison', 'already', 'big', 'hiking', 'sells', 'thin', 'cruel', 'belong', 'blow', 'sync', 'oysters', 'cats', 'wounded', 'dark', 'seize', 'intruding', 'animals', 'adorable', 'calls', 'zealots', 'respects', 'paints', 'face', 'cooking', 'pain', 'elk', 'son', 'fit', 'abandon', 'be', \"'s\", 'bogus', 'discreet', 'also', 'designed', 'liar', 'blue', 'join', 'bossy', 'lighten', 'jump', 'sick', 'tries', 'snoring', 'lives', 'beware', 'team', 'romantic', 'work', 'game', '99', 'dump', 'fine', 'starved', 'thirsty', 'faster', 'felt', 'nearby', 'video', 'agent', 'touch', 'timing', 'piano', 'aim', 'gross', 'viral', 'rice', 'running', 'stuff', 'admit', 'raining', 'sympathize', 'light', 'fatal', 'dumb', 'low', 'fire', 'minute', 'fought', 'money', 'arm', 'hiccups', 'flip', 'taller', 'wonder', 'wash', 'queen', '!', 'plastered', 'exhaled', 'sneeze', 'cookies', 'suppose', 'hiring', 'fall', 'bragging', 'problem', 'chair', 'horse', 'prefer', 'apologize', 'visa', 'thanks', 'runner', 'unlikely', 'hungry', 'asian', 'purr', 'liked', 'hives', 'rose', 'french', 'trapped', 'tidy', 'great', 'ai', 'going', 'famished', 'ambitious', 'some', 'drives', 'dizzy', 'impatient', 'answers', 'hiding', 'loved', 'silent', 'cheats', 'china', 'caused', 'brave', 'hot', 'huge', 'told', 'skate', 'step', 'beans', 'farmer', 'approves', 'old', 'paint', 'buying', 'heal', 'secret', 'tenure', 'news', 'trainee', 'trying', 'outrank', 'nights', 'comes', 'japanese', 'gorgeous', 'impulsive', 'hurried', 'home', 'justice', 'quiet', 'blind', 'exist', 'made', 'fly', 'thrilled', 'number', 'dropped', 'coward', 'dies', 'washed', 'this', 'borrow', 'spot', 'there', 'frowning', 'restless', 'myself', 'joking', 'sweated', 'sad', 'can', 'win', 'mail', 'welcome', 'sit', 'eggs', 'yen', 'hop', 'kite', 'bless', 'scared', 'apples', 'wrong', '30', 'call', 'lost', 'invited', 'vision', 'scammed', 'easy', 'garbage', 'hit', 'sell', 'gag', 'relaxed', 'goodbye', 'takes', 'set', 'mocked', 'bag', 'damaged', 'smiled', 'box', 'untrue', 'garlic', 'parody', 'worse', 'awesome', 'stunned', 'shower', 'green', 'safely', 'cough', 'rebuild', 'works', 'release', 'denied', 'adult', 'cutie', 'group', 'violent', 'name', 'surviving', 'pitch', 'elected', 'pie', 'lawyer', 'argue', 'bark', 'push', 'friends', 'listen', 'member', 'ship', 'carded', 'homesick', 'eating', 'free', 'screamed', 'which', 'reading', 'bribed', 'doubts', 'resilient', 'makes', 'fun', 'shouting', 'cat', 'brief', 'lines', 'feel', 'alone', 'snowed', 'ruthless', 'owe', 'motivated', 'weak', 'movies', 'possible', 'prove', 'both', 'father', 'year', 'lies', 'alert', 'humming', 'strict', 'fooled', 'nobody', 'away', 'smoke', 'shame', 'star', 'lawyers', 'rescued', 'jobs', 'insist', 'foot', 'surprised', 'angry', 'resist', 'cute', 'page', 'cd', 'wet', 'plant', 'creative', 'lie', 'fed', 'classes', 'obey', 'woman', 'vulgar', 'today', 'raise', 'sensible', 'course', 'trust', 'rid', 'obsolete', 'grounded', 'amnesia', 'rewrote', 'choice', 'hero', 'follow', 'please', 'and', 'mess', 'dead', 'turn', 'followed', 'below', 'obvious', 'grew', 'before', 'counts', 'rental', 'word', 'exciting', 'bed', 'jokes', 'seems', 'own', 'enemies', 'russia', 'mistaken', 'scolded', 'saved', 'dry', 'survived', 'absent', 'once', 'years', 'lamp', 'jerk', 'smells', 'sweets', 'sink', '%', 'refuse', 'skiing', 'watchful', 'gloat', 'relax', 'drinks', 'put', 'hand', 'hope', 'close', 'resigned', 'divorced', 'song', 'hated', 'believe', 'rescheduled', 'livid', 'escaped', 'point', 'all', 'heard', 'tan', 'decide', 'amuse', 'have', 'ice', 'diet', 'wore', 'warn', 'choose', 'cookie', 'puzzles', 'hell', 'cancer', 'yelled', 'tomorrow', 'idea', 'suits', 'clever', 'spiders', 'dope', \"'d\", 'sober', 'him', 'fan', 'buy', 'annoying', 'rational', 'reasonable', 'yes', 'closed', 'convinced', 'matters', 'lying', 'she', 'cds', 'upstairs', 'voted', 'attentive', 'fill', 'got', 'protest', 'way', 'shock', 'could', 'sign', 'circle', 'coughed', 'risk', 'sand', 'ask', 'flabby', 'opera', 'active', 'coke', 'really', 'cities', 'he', 'uninsured', 'fret', 'cool', 'hurts', 'jelly', 'spies', 'confident', 'tea', 'tall', 'genius', 'bled', 'gasps', 'depressed', 'had', 'obeys', 'owes', '17', 'gate', 'sued', 'proof', 'shut', 'london', 'sincere', 'listening', 'check', 'simple', 'ruined', 'many', 'support', 'stinks', 'out', 'anything', 'healthy', 'gun', 'phony', 'book', 'concerned', 'any', 'karaoke', 'odd', 'expelled', 'involved', 'excuse', 'personal', 'cultured', 'jazz', 'found', 'midnight', 'insects', 'baking', 'body', 'urgent', 'worry', 'dancer', 'happy', 'squinted', 'special', 'nap', 'dentist', 'teasing', 'expecting', 'resent', 'cooks', 'fad', 'card', 'hard', 'swim', 'eat', 'available', 'do', 'prepared', 'talk', 'sunk', 'snowing', 'job', 'who', 'nodded', 'embraced', 'wealthy', 'hugged', 'line', 'unwell', 'dating', 'wins', 'cheers', 'worried', 'strange', 'last', 'hide', 'actor', 'stay', 'hey', 'beard', 'approve', '8:30', 'carry', 'fired', 'gawking', 'plan', 'useless', 'messed', 'forget', 'aside', 'liars', 'night', 'uncle', 'surrendered', 'use', 'very', 'her', 'drag', 'tourist', 'force', '2:30', 'shoot', 'unbiased', 'again', 'speak', 'agree', 'must', 'doubt', 'unfair', 'upstate', 'ufo', 'happens', 'firefox', 'crowds', 'begin', 'later', 'worrying', 'honey', 'exaggerated', '7:30', 'ladies', 'wish', 'ride', 'blame', 'cured', 'teacher', 'kidding', 'twin', 'blushed', 'amazed', 'delighted', 'deserved', 'means', 'caution', 'strong', 'curious', 'picnics', 'dance', 'enjoyed', 'so', 'orphan', 'gentle', 'stood', 'new', 'greedy', 'sweet', 'sisters', 'saint', 'grab', 'write', 'everything', 'let', 'always', 'bit', 'fight', 'stuffed', 'air', 'usually', 'warm', 'disgusted', 'deny', 'committed', 'mean', 'of', 'celery', 'treat', 'believed', 'forgetful', 'fast', 'wept', 'behave', 'surgery', 'boat', 'yacht', 'awful', 'knife', 'swam', 'hired', 'died', 'best', 'up', 'pay', 'desperate', 'enter', 'poet', 'tastes', 'talented', 'hang', 'birthday', 'flu', 'touched', 'boys', 'agreed', 'mahjong', 'respect', 'voting', 'drop', 'nothing', 'graduated', 'weird', 'summer', 'inhaled', 'grouch', 'autumn', 'dvd', 'rules', 'gambling', 'leaving', 'guessed', 'oven', 'muslim', 'boston', 'thank', 'locked', 'twins', 'obedient', 'lasagna', 'dieting', 'cheat', 'doctor', 'life', 'seen', 'sass', 'hassle', 'happened', 'teach', 'sleep', 'pick', 'working', 'clearly', 'understood', 'cursed', 'ears', 'kids', 'to', 'suntan', 'fiasco', 'tired', 'bottoms', 'respectful', 'shadow', 'half', 'quite', 'bring', 'paying', 'excited', 'list', 'facts', 'hung', 'sirens', 'winning', 'camera', 'eaten', 'flat', 'complex', 'maybe', 'sue', 'plead', 'should', 'wake', 'called', 'things', 'nasty', 'fruit', 'bread', 'student', 'sexy', 'monk', 'return', 'fasting', 'queasy', 'surfer', 'driving', 'waved', 'corrected', 'lame', 'food', 'fainted', 'assume', 'whistled', 'heroic', 'draw', 'snow', 'shot', 'trusted', 'pass', 'cab', 'single', 'tell', 'ate', 'helps', 'fell', 'ill', 'bite', 'secrets', 'burn', 'bat', 'form', 'moving', 'blood', 'send', 'rested', 'moody', 'town', 'rocks', 'split', 'bored', 'fixed', 'sneaky', 'occupied', 'stubborn', 'hip', 'people', 'near', 'everyone', 'like', 'along', 'faking', 'rifle', 'poems', 'smoking', 'immune', 'one', 'arrived', 'disagree', 'exercise', 'here', 'staring', 'sugar', 'alive', 'protested', 'lazy', 'confessed', 'diabetic', \"'re\", 'fair', 'beats', 'oh', 'advice', 'bird', 'act', 'god', 'bell', 'age', 'saturn', 'asleep', 'move', 'over', 'day', 'wo', 'silly', 'safe', 'honest', 'rule', 'crude', 'respond', 'by', 'cringed', 'sickens', 'packing', 'still', 'ugly', 'sashimi', 'weapon', 'peace', 'count', 'full', 'thinking', 'framed', 'plants', 'rad', 'sat', 'certain', 'mouse', 'lips', 'chat', 'drive', 'quickly', 'succeeded', 'starve', 'cried', 'bald', 'paris', 'hands', 'panting', 'key', 'party', 'much', 'strike', 'too', 'shopping', 'ski', 'winter', 'shovel', 'thief', 'blessed', 'skating', 'just', 'grateful', 'diary', 'rich', 'girls', 'expert', 'golf', 'realistic', 'author', 'english', 'cpr', 'mortal', 'seated', 'prepaid', 'jogging', 'safer', 'above', 'sure', 'qualified', 'salmon', 'breath', 'hug', 'confused', 'history', 'castles', 'guilty', 'part', 'ironic', 'cops', 'artist', 'college', 'japan', 'walk', 'obeyed', 'bananas', 'superb', 'dig', 'chuckled', 'noisy', 'ignored', 'hi', 'glad', 'that', 'r', 'priest', 'war', 'poetry', 'crazy', 'model', 'tense', 'observant', 'mock', 'powerful', 'somebody', 'shoe', 'miss', 'bike', 'true', 'broke', 'nailed', 'iron', 'coffee', 'jittery', 'poor', 'losing', 'coach', 'leg', 'italian', 'fool', 'mask', 'spy', 'insecure', 'coat', 'dozed', 'leave', 'rest', 'normal', 'suicidal', 'annoy', 'ticklish', 'give', 'likes', 'know', 'yelling', 'gold', 'cousins', 'focus', 'korean', 'retired', 'tv', 'tempted', 'yawned', 'swore', 'donut', 'save', 'hate', 'your', 'popular', 'come', 'slept', 'everybody', 'meant', 'cheap', 'eats', 'fix', 'lonely', 'stupid', 'crime', 'marry', 'parole', 'pale', 'beside', 'rebel', '&', 'contact', 'keep', 'show', 'american', 'mad', 'calm', 'pardon', 'pool', 'yourself', 'legal', 'asap', 'threw', 'passed', 'writing', 'haircut', 'share', 'banana', 'sloshed', 'staying', 'make', 'naked', 'start', 'flew', 'hates', 'pony', 'peas', 'ghosts', 'wine', 'reformed', 'around', 'next', 'play', 'falling', 'pen', 'awake', 'sunsets', 'pregnant', 'hiccup', 'speaking', 'upset', 'seal', 'try', 'soaked', 'deserve', 'lucky', 'look', 'luck', '.', 'stayed', 'impressed', 'drown', 'turned', 'red', 'knows', 'in', 'struggled', 'outlaw', 'avoids', 'wants', 'relief', 'studying', 'deer', 'is', 'slacker', 'broken', 'comment', 'approved', 'deep', 'shooting', 'down', 'back', 'gullible', 'scam', 'loser', 'terrific', 'went', 'hunk', 'foolish', 'arguing', 'dinner', 'painful', 'tease', 'horses', 'girl', 'shouted', 'checked', 'kobe', 'take', 'orders', 'tokyo', 'bonus', 'talked', 'leak', 'math', 'spoke', 'chicken', 'bummer', 'smile', 'late', 'enjoy', 'dug', 'oppose', 'patient', 'panic', 'shrieked', 'dressed', 'help', 'lot', 'mug', 'catch', 'space', 'loaded', 'biased', 'decline', 'love', 'does', 'listens', 'snob', 'cook', 'about', 'walked', 'third', 'good', 'bright', 'resentful', 'think', 'two', 'someone', 'borrowed', 'clear', 'better', 'ran', 'wise', 'aboard', 'plans', 'ignore', '100', 'hesitated', 'engaged', 'beef', 'open', 'beaten', 'finicky', 'lead', 'taxes', 'change', 'how', 'baffled', 'muffins', 'ball', \"'m\", 'buried', 'succeed', 'shaken', 'eighteen', 'skinny', 'crafty', 'writer', 'proceed', 'arabs', 'for', 'ranch', 'flaw', 'naive', 'faithful', 'important', 'sec', 'changed', 'thirty', 'may', 'canceled', 'bet', 'pretty', 'gift', 'futon', 'puzzled', 'sushi', 'differ', 'rumor', 'forgive', 'rain', 'smashed', 'fox', 'shocked', 'stir', 'soon', 'missing', 'why', 'feels', 'lip', 'anyone', 'fearless', '5', 'survive', 'forbid', 'boss', 'italy', ',', 'guts', 'ex-con', 'on', 'sensitive', 'begun', 'bath', 'bilingual', 'dumped', 'drowning', 'accept', 'books', 'bulky', 'adaptable', 'outdated', 'fishing', 'improvised', 'or', 'dare', 'monday', 'easily', 'lion', 'cars', 'apologized', 'man', 'business', 'yellow', 'tight', 'acquired', 'complain', 'kicked', 'humble', 'lit', 'pathetic', 'loosen', 'miserable', 'security', 'juggle', 'stalled', 'reborn', 'copy', 'wrote', 'built', 'brought', 'looks', 'thorough', 'mine', 'sighed', 'find', 'almost', 'gambler', 'as', 'rights', 'ironing', 'singer', 'nice', 'pig', 'jesuit', 'rude', 'a', 'arrested', 'chubby', 'quietly', 'spinach', 'paid', 'clocks', 'needs', 'pity', 'nauseous', 'black', 'steal', 'art', 'will', 'drew', 'might', 'van', 'cry', 'anybody', 'filming', 'loyal', 'toys', 'morning', 'despise', 'something', 'hoax', 'quit', 'refused', 'together', 'dedicated', 'doctors', 'fake', 'horrible', 'read', 'mind', 'me', 'wow', 'gazed', 'shake', 'shoes', 'milk', 'curt', 'slowly', 'test', 'grapes', 'myth', 'joke', 'child', 'tom', 'curse', 'gamble', 'snack', 'lobster', 'henpecked', 'armed', 'volunteered', 'you', 'cares', 'talking', 'nervous', 'done', 'grow', 'floor', 'tigers', 'ink', 'panicked', 'psyched', 'unmarried', 'them', 'humor', 'warned', 'fact', 'often', 'won', 'purist', 'cheer', 'included', 'us', 'now', 'dirty', 'winners', 'crying', 'cared', 'wife', 'pun', 'stutters', '3:30', 'faint', 'deal', 'moaned', 'cut', 'pleased', 'turtles', 'gossip', 'stamp', 'white', 'no', 'promised', 'unbelievable', 'boring', 'first', 'stand', 'innocent', 'looking', 'sinking', 'busy', 'kept', 'became', 'questions', 'offended', 'shrugged', 'are', 'sold', 'lunch', 'parties', 'inside', 'small', 'through', 'his', 'soccer', 'foggy', 'fail', 'my', 'genuine', 'noticed', 'kid', 'decorated', 'friend', 'rock', 'intrigued', 'freezing', 'clean', 'recovered', 'kill', 'aches', 'senior', 'came', 'careful', 'we', 'fussy', 'envy', 'type', 'truck', 'ought', 'lose', 'holidays', 'drunk', 'gas', 'decided', 'hers', 'flying', 'tree', 'happen', 'truthful', 'killed', 'nearly', 'saying', 'drowned', 'pack', 'taste', 'cattle', 'risks', 'baker', 'smokes', 'phoned', 'poison', 'not', 'games', 'scare', 'uneasy', 'missed', 'run', 'understand', 'pray', 'polite', 'see', 'predicted', 'generous', 'needed', 'apart', 'live', 'obese', 'grumpy', 'the', 'moment', 'flunk', 'pinched', 'tripped', 'rope', 'revenge', 'high', '$'}\n",
            "Target Vocabulary {'vie', 'bibi', \"t'asseoir\", 'petit', 'sauve-toi', 'terminé', 'trains', 'médecins', 'donner', 'livide', 'pinceaux', 'attaquerons', 'pourquoi', 'trouvai', 'garantis', 'dis-le', 'saute', 'lancer', 'entretenues', 'adoré', 'priorité', 'prête', 'prêtes', 'violon', 'dehors', 'petites', 'soupira', 'démarrée', 'six', 'cessez', \"j'eus\", 'tes', 'étais-je', 'gelée', 'chançarde', 'dis-moi', \"n'entrez\", 'file', 'heureuse', 'risques', 'ténu', 'après', 'lieu', 'pige', 'l', 'jouis', 'tienne', \"d'aide\", 'plantés', 'au', 'imitation', 'claque', 'rêves', 'soin', 'reposez-vous', 'bisou', 'combien', 'continué', 'regardez-le', 'voleur', \"d'attendre\", \"m'as\", 'écoutez', 'procès', 'élue', \"l'appeler\", 'stagiaire', 'arrivée', 'emprunté', 'boulanger', 'jongler', 'désorientée', \"qu'elle\", 'tirez', 'remués', 'assure', 'irez-vous', 'objection', 'détendue', 'trouverai', 'vote', 'ailles', 'amical', 'vend', 'irez', 'manteau', 'cœur', 'danse', 'connait', 'parlé', 'enfoirée', 'avais-je', 'saisis-toi', 'peux', 'saoul', 'sont', 'amende', 'vue', \"s'agit\", 'acheter', 'chambre', 'impulsif', 'sais', 'troisième', 'peinture', \"qu'un\", 'fais', 'meurs', 'table', 'immédiatement', 'repos', 'bouffé', 'interrompre', 'malaise', 'lui', 'entraîné', 'skier', 'dépourvu', 'préoccupe', 'commencerai', 'erreur', 'leur', 'pleure', 'amusée', 'ok', 'étudiant', \"l'avez\", 'beau', '’', 'amoureux', 'assieds-toi', 'matinale', \"l'italie\", 'retournez', 'muera', 'dentaires', 'poursuivez', 'aida', 'pis', 'défaits', 'cadeau', 'avons-nous', 'pile', 'surpris', \"j'expliquerai\", \"n'êtes\", 'nia', \"l'as-tu\", 'sens', 'ivre', 'battu', 'frappée', 'dupa', 'date', 'seule', 'difficile', 'bouge', 'ignoré', 'sortis', 'ça', \"l'a\", 'vieillot', 'nageaient', 'ci-dessous', 'recrutes-tu', 'où', 'conçu', 'regarde-t-il', \"t'ai-je\", 'ennemies', 'tendu', 'saoules', 'moque', 'salut', 'conneries', 'haï', 'marqué', 'dj', 'liquide', 'boîte', 'condamnés', 'vous-même', 'jubilez', 'vivons', 'jambon', 'deviné', 'jouets', 'rougi', \"j'apprendrai\", 'préviens', 'enrhumé', 'maigrichons', 'dessous', 'captes', 'causer', 'lundis', 'mouillé', 'navire', 'ferons-nous', 'importe-t-il', 'nord', 'égalité', 'entendre', 'poussez-vous', 'devine', 'égard', 'essaye', 'désolées', 'monté', 'appartient-il', \"d'arrêter\", 'partiale', 'araignées', 'flouée', 'ressentiment', 'obéira', 'désaccord', 'accises', 'clef', 'rouspéteur', 'veille', 'nie', 'trompé-je', 'compter', 'serait-ce', 'cria', 'tenue', 'préviendrai', 'travailler', 'prend', 'brillant', 'cuire', 'bouchée', 'endormis', 'endroit', 'tourne', 'complexe', 'apportez-le', 'intrigué', 'pouvions', 'esseulée', 'essayons', 'minable', 'utilisez', 'veillé', 'sortie', 'donne', 'bougez', 'fac', 'touchée', 'chaude', 'donne-le-moi', 'abandonne', 'printemps', 'sauter', 'reproche', 'mèche', 'épaules', 'poil', 'notre', 'espèce', 'ignorée', 'peut-être', 'compris', 'ému', 'heure', 'présentons', 'dit', 'mary', 'dépêchée', 'rouges', 'dormir', 'viendra', 'impatiente', 'bigot', \"j'aiderai\", 'harcelé', 'avis', 'coca', 'regardé', 'dors', \"d'étudier\", 'excité', 'magnat', 'mentons', 'flasque', 'avocate', 'couillon', 'signez', 'entraîne', 'dessiné', 'neutre', \"j'exhorte\", 'demandons', 'contente', 'passes', 'formons', 'nature', 'souviens', 'grandiront', 'quiconque', 'fainéant', 'sentir', \"l'or\", 'emplois', 'ennuie', 'ecrivez', 'boulet', 'scellez', 'motivé', 'divorcée', 'buvez', \"s'entend\", 'sectaire', 'aimé', 'endormies', 'revoilà', 'bombe', 'sourd', 'poursuivi', 'bénit', 'gardez-le', 'préfère', 'question', 'sidérée', 'facile', 'recalée', 'créatifs', 'satisfaits', 'savez-vous', 'tirait', 'restes', 'montrez-moi', 'marcherai', 'tranquilles', 'barbe', 'grippe', \"d'eau\", \"l'évidence\", 'marrants', 'mot', 'mignon', '«', 'peut-on', 'poulets', 'veux', 'prenez', 'paye-lui', 'ouvrez', \"jusqu'ici\", 'camionnette', 'passerai', 'fuite', 'emmène', 'h', 'décédées', 'dorment-ils', 'douleur', 'veinardes', 'folle', 'vais', 'prendrai', 'bien', 'à', 'saisis-moi', 'brûlés', \"j'entends\", 'choqués', 'méticuleux', 'dégagez', 'céleri', \"l'espère\", 'arrêter', 'manières', 'gâteau', 'laisse-toi', 'désorienté', 'filai', 'lâche-toi', 'puissant', 'mien', 'malin', 'aversion', 'nettoierai', 'revoyure', 'énervé', 'promettez-vous', 'pénétré', 'éloigné', \"j'entrerai\", 'fiancés', 'suivis', 'mettez', 'pouvais', 'ses', 'ronronnent-ils', 'et', 'perdus', 'personne', 'plaisanteries', 'suis-moi', 'fonctionne', 'peint', 'proches', 'tenterai', 'promets-tu', 'gavée', \"d'immondices\", 'demie', 'observateur', \"l'histoire\", 'discuté', 'prudent', 'bouffée', 'ignore-les', 'tirerai', 'blagues', 'nettoyez', \"j'imagine\", 'rejoindre', 'grandes', 'tien', 'décidez', 'servez-vous', 'laissez-le', 'couru', 'diète', 'journal', 'biaisé', 'devinez', 'autour', 'blessées', 'dirai', 'morte', \"d'entrer\", 'silencieux', \"j'irai\", 'vois-tu', 'neuf', 'être', 'danseur', 'gavées', 'taquinez', 'devais', 'aimions', 'plais', 'tenterons', \"n'y\", 'plombs', 'correct', 'envoie-le', 'apprêtés', 'plaisantez', 'emmerdeuse', 'accepté', 'perdons', 'bleu', 'ouïe', 'enterré', 'décédé', 'libérez', 'utilisez-les', 'alitées', 'corps', 'suffisant', 'repose', 'b', 'nagerons', 'relaxe', 'soif', 'eu', \"n'est-il\", 'super', 'loup', 'ruinée', 'barbant', 'bougon', 'soyez', 'chante', 'clouée', 'jalouses', 'souciait', 'risque-t-il', 'extraverti', 'admire', 'dites', 'enfuit', 'tiendrai', 'domicile', \"m'aider\", 'débarrasse-toi', 'rêve', 'pourvu', 'diplôme', 'déteste', 'arrête', 'prêtez-moi', 'souhaitez-moi', 'ri', 'thune', 'pagaille', 'entrer', 'restâmes', \"j'essayai\", 'sobre', 'piscine', 'menti', 'effrayé', \"d'amnésie\", 'beignet', 'nourriture', 'dommage', 'arrières', 'sale', 'possède', 'sournois', 'excuse-moi', 'enfant', 'fiez', 'camouflez-le', 'trace', 'ordonnée', 'attends', 'pincée', 'pourrions-nous', 'porté', 'sieste', 'verbaliser', 'justes', 'reçu', 'satisfait', 'commentaire', 'soleil', 'fruits', 'fatiguées', 'rendras-tu', 'joue-la', 'contrat', 'serre-moi', \"m'ennuyais\", 'mec', 'agace', 'fumer', 'permettrai', 'éclipsé', 'gratuit', 'nouvelle', 'décidé', 'impitoyables', 'disputé', 'pratiquez-vous', \"m'ennuyer\", 'courses', 'dingues', 'sournoises', 'fallu', \"n'ira\", 'sent', '50', 'excellent', 'jésuite', 'poker', 'jaune', 'cuisinez', 'voir', 'terre', 'décroché', 'assez', 'inquiétez', 'détestent', 'palpitant', 'éveillés', 'ignorez', \"m'amuses\", 'trucs', 'honoré', 'las', \"l'intérieur\", 'chaussure', 'diabétique', 'elles', 'cheveux', 'notes', 'lapidé', 'dérangez', 'respecte', 'un', 'ferme', 'attrape', 'certaine', \"j'adore\", 'poursuivit', 'voler', \"t'es\", 'avise-moi', 'donnez-moi', 'restons', 'présent', 'en-cas', 'tort', 'demande', 'taisez-vous', 'échauffe-toi', 'vomi', 'terminer', 'suis-nous', 'maigrichonnes', \"d'y\", 'thé', 'bâti', \"j'essaierai\", 'contrefaçon', 'meilleur', 'mourrons', 'baissé', 'jura', \"j'écoute\", 'changerai', 'musique', 'mesure', 'réveillées', 'divisons-nous', 'terrible', 'phénomène', 'promîmes', 'sortir', 'bailla', \"t'as\", 'laissez-nous', 'là', 'skie', 'saouls', 'mesquin', 'alors', 'taillez-vous', 'armées', 'prudents', \"qu'en\", \"t'ennuie\", 'noyer', 'laid', 'positive', 'vacances', 'perruque', 'arrivera', 'soutiens', 'approchez', 'gagnâmes', 'verrouillez', \"n'allez-vous\", 'ensuite', 'bougea', 'partis', 'hongrois', 'côté', 'acheté', \"l'apprécions\", 'hait', 'finir', 'suivie', '?', 'amendée', 'geindre', 'demeurez', \"j'abandonne\", \"n'arrive\", 'célébrité', 'offre-moi', 'visible', 'ouvert', 'doit', 'réveille-toi', 'rien', 'offre', 'formidable', 'pensé', 'disjoncté', 'sain', 'commencer', 'quitte', 'long', 'achète-toi', 'nouveaux', 'bon', 'brouillard', 'froncer', 'tenter', 'danger', 'poussent', 'viande', 'effort', 'suivi', 'lavez-vous', 'resté', 'levez', 'coin', 'fric', 'aviez', 'réveille', 'piqué', 'tristes', 'livre', 'trouvés', 'attirante', 'était-il', 'voilà', 'entraînée', 'méchant', 'dix-sept', 'nagerai', 'giflée', 'vertiges', 'congé', 'blanc', 'détendues', 'dansons', 'reste', 'prends-le', \"t'échauffer\", 'haricots', 'bourrée', 'dures', 'vélo', 'dégoûté', \"t'y\", 'renversé', 'ballon', 'alla', 'piégées', 'flippé', 'patin', 'mes', 'pelle', 'commencez', 'annuler', 'équitables', 'décroche', 'arrêtée', 'têtu', 'chance', 'impliquée', 'captez', 'permets-moi', \"qu'on\", 'forme', 'affairée', 'somme', 'vit', 'jambe', 'paresseux', 'prudemment', 'célibataire', 'disputée', 'cérébral', 'recruté', 'suicidaire', 'tour', 'dis', 'guérie', 'sucre', 'éclatâmes', 'partons-nous', \"d'avance\", 'parler', 'prêt', 'fortunée', 'noya', 'obéissant', 'roi', 'iras-tu', 'résister', 'savais', 'éveillées', 'iras', 'clés', 'curieux', \"m'aimes-tu\", 'choses', \"s'adaptera-t-il\", \"n'a\", \"m'avez\", 'copié', 'choc', 'amie', 'causé', 'dément', 'pressez-vous', 'montrez-vous', 'foyer', 'obéi', 'connu', 'cachons', 'détails', 'appelle-nous', 'ainsi', 'attentif', 'fixer', 'pardonne', 'sali', 'poursuis', 'métier', 'expiré', 'autre', 'clémentes', \"j'interdis\", 'comprenons', 'sang', 'hors-la-loi', 'pourrions', 'tombez', \"n'aime\", 'populaire', 'appareil', 'essayé', 'prendre', 'arrivent', 'poèmes', 'tenace', 'malines', 'allez-y', 'proche', 'inspiré', 'lire', 'lève-toi', 'apprends', 'rentré', 'conjure', 'met', \"n'es\", \"t'adore\", 'chiqué', 'marie', 'effectué', 'ailleurs', 'bonsoir', \"t'avons\", 'lentement', 'donc', 'coupée', 'formuler', 'creusé', 'répondez-moi', 'élu', \"j'apprécie\", 'parlez', 'cupide', 'échauffez-vous', 'donné', 'vers', 'détendus', 'apporte', 'véhiculée', \"m'intrigue\", 'vendues', 'fainéantes', 'attrapée', \"l'avons\", 'nager', 'soucier', \"t'apprécie\", 'mettre', 'repu', 'gelé', \"l'a-t-elle\", 'mille', 'patron', 'sucreries', 'venons', 'vide', \"l'acquisition\", 'déciderai', 'embrassez-moi', \"d'être\", 'géré', 'devrions-nous', 'mauvaises', 'reposé', 'hurler', 'choisissez', 'énorme', 'normales', 'britannique', 'fortuné', 'lumière', 'battez', 'vas', 'crierai', 'voyant', 'preuves', 'capable', \"l'aide\", 'choix', 'synchronisés', 'regardez-nous', 'aussi', 'lâche', 'suivrons', 'sauvées', 'arrête-toi', 'bête', 'prévenue', 'bus', 'gamin', 'sortirai', 'promenade', 'mange', 'casse-toi', 'guérira', 'impertinent', 'touillez', 'loyale', 'contravention', 'bonnes', 'oses-tu', \"j'honore\", 'impartial', 'stop', 'excursion', 'caviar', 'oui', 'trahi', 'content', 'sérieuse', 'vieilles', 'près', 'combattrons', 'effet', 'gras', 'roulez', 'dirent', \"n'entendîmes\", 'normaux', 'lâchez', 'armes', 'chez', 'connue', 'riche', 'allons-nous', \"n'irai\", 'mélange', 'valises', 'mah-jong', \"m'abstiens\", 'suivez-le', 'fantômes', \"s'il\", 'gênez', 'poney', 'contrariées', 'tatillon', 'crue', 'attaquez', 'pouvoir', 'sentais', \"l'addition\", 'ne', 'attrapez', 'bord', 'vérifie', 'sauverai', 'jetez', 'satisfaites', 'idiot', 'partirais', 'creuse', 'vengeance', 'risque', 'tellement', 'dévouée', 'mais', 'pêchez', 'cinglé', 'photo', 'survivre', 'par', 'démissionne', 'toi', 'marrés', '19', 'marrantes', 'œil', \"t'aime\", 'appelée', 'sourire', 'rêver', 'pensionné', 'malheureux', 'naïfs', 'tournai', 'partit', 'premier', 'japon', 'démodé', 'immobiles', 'corde', 'tricote', 'tira', 'donnent', 'goûte-le', 'généreux', 'amoureuses', 'couchers', 'le', 'détends-toi', 'écrit', 'sautez', 'suicide', 'réussi', \"t'admire\", 'prison', 'fais-moi', 'lunatique', 'bégaie', 'chantent', 'impartiale', \"t'embête\", 'ovni', 'rusées', 'maigrichon', 'intelligents', 'choisis-en', 'beaux', 'professeur', 'protège', \"s'en\", 'marcherons', \"s'envole\", 'collant', 'faisait', 'cruel', 'sens-tu', 'malade', 'clément', 'augmente-le', 'pleut-il', 'vivant', 'mérité', 'malchanceuse', 'quand', 'reviendrai', 'souffrons', 'sept', 'folles', 'ta', 'continuez', 'dégage', 'sembles', 'est-il', \"j'aime\", 'brillantes', 'égoïste', 'regardez', 'épousez-moi', 'normale', 'bois-le', 'événement', 'minuit', 'voyage', 'arnaquée', 'grand', 'adorable', \"j'obtiendrai\", 'loi', 'manière', 'sournoise', 'givrée', 'rester', 'face', 'éclaté', 'secouée', 'pain', 'son', 'retourne', 'fit', 'volé', 'buvons', 'du', 'cause', 'fous', 'oiseaux', 'prune', \"d'ici\", 'ont', 'ouvre-moi', 'armé', 'prions', 'attendons', 'sortons', 'floué', 'respectueux', 'vraiment', 'm', 'ruiné', 'obéit', 'sentait', 'lis', 'protégez', 'gagné', 'les', 'chanté', 'affaiblie', 'parti', '99', 'brisé', 'dégoûtée', 'mots', 'ralentis', 'patronne', 'détendu', 'étourdie', 'couteau', 'bananes', 'allons-y', 'chauve', 'cinq', 'supplie', 'descendez', 'poursuit', 'attrapées', \"d'amis\", 'trouva', 'avec', 'lasagnes', 'déciderons', 'j', 'agent', 'vérifier', 'quittés', \"qu'ai-je\", 'comportez-vous', 'coupable', 'piano', 'remis', 'écris', 'trouvais', 'devins', 'aucune', 'compte', 'mangerai', 'changé', 'lacet', 'serais', 'instant', 'lisons', 'mangeons', 'fie', 'alitée', 'fatal', 'alentour', 'arrêté', 'montres', 'laisse', 'journée', 'minute', 'crier', 'votre', 'porte-le', 'démontez-moi', 'démarre', 'acteur', 'touché', 'mise', 'chanterai', 'sérieuses', 'sauf', 'raconté', 'miséricordieuse', 'demi-tour', 'vanné', 'demander', 'cocufié', 'immunisé', \"l'ouvrirai\", '...', 'prévenus', 'incorrect', 'sans', 'pourrait', 'parfait', 'fonctionné', 'apeuré', 'hoquet', 'coule', 'gâchis', '!', 'sociable', 'triste', 'contrariés', 'max', 'disque', 'demain', 'trouille', 'léger', 'cookies', 'suppose', 'distinctement', 'entre', 'râlé', \"m'évanouir\", 'voyante', 'cuisinier', 'veux-tu', 'gagnèrent', \"n'achète\", 'apprêtées', 'honnêtes', 'séché', 'sourde', 'biscuits', 'nettoyer', 'fils', 'utilise-les', 'visa', 'tuer', 'savaient', 'bras', 'peur', 'commençons', \"t'admirons\", \"j'aimerais\", 'confiance', 'grimpe', 'artiste', 'jeune', 'pari', 'pesé', 'joue', 'plaisantons', 'accroche-toi', 'alités', 'forces', 'année', 'retire-toi', 'frappé', 'rose', 'contrariée', 'sont-elles', \"n'est\", 'ferme-la', 'donne-le-lui', 'ai', 'cruelles', 'convaincu', 'préparée', '\\n', 'croque', 'signe', 'sécurité', 'jette', 'biser', 'impatient', 'satisfaite', 'jeux', 'matinal', 'portée', 'vieux', 'halte', 'espion', 'manqué', 'plaide', 'faites-lui', 'mettez-vous', 'dois-je', 'numéro', 'poisson', 'brave', 'astucieux', 'excitée', 'arrêtons', 'mon', 'tout', 'lavée', 'rencontré', \"quelqu'un\", 'canular', \"m'échappe\", 'recule-toi', 'attrapez-le', 'avancez', 'disponible', 'anglais', 'cire', 'nez', 'improbable', 'cousines', 'secret', 'fille', \"d'argent\", 'vendus', 'arrives-tu', 'plein', 'cultivée', 'incluse', 'es', 'épouse-moi', 'touche', 'affolé', 'actuellement', 'plierai', 'cheminé', 'cuisiné', 'écrire', 'manque-t-elle', \"s'emmerde\", 'jumelle', 'équipe', \"d'eux\", 'mental', 'fie-toi', 'suis-je', 'impulsive', 'obstiné', 'débrouiller', \"l'erreur\", \"l'essayerai\", 'plombier', 'conduis-toi', 'justice', 'canadien', \"aujourd'hui\", 'est', 'regarde-nous', 'laide', 'plaisance', 'su', 'nue', 'rcp', \"m'adapte\", 'positif', 'bienvenue', 'pèse', 'pitié', 'eue', 'rompre', 'mémorise-le', 'qualifié', 'siège', 'demandons-lui', 'boit', 'aiderai', 'chatte', 'brillante', 'créatives', 'aima', 'moins', 'preuve', 'débrouillerons', 'sien', 'revenu', \"l'utilise\", \"l'entraîneur\", 'amoureuse', 'empruntée', 'sers', 'nettoie-le', 'poissons', 'astucieuse', 'viens-tu', 'prends', 'baissai', 'lait', 'sou', 'inspira', 'échappés', 'daube', \"n'ont\", 'vaches', 'protestation', 'amour', 'tatillonne', 'joli', 'amusé', 'rebondissons', 'forte', \"l'état\", 'plantées', 'gratuitement', 'eut', 'joyeuse', '»', 'initiée', 'fallait', 'chienne', 'arrière', \"j'espère\", 'frappa', 'spectacle', 'gauche', 'con', 'existent', 'gaspillage', 'yen', 'gnangnan', 'siens', \"l'ignore\", 'cerf-volant', 'mit', 'rencontrés', 'dur', 'désolé', 'dentiste', 'pouvez', 'tripes', 'binoclard', '30', 'occupée', 'corrigez', 'demandé', 'assisterai', 'en', 'cas', 'fermer', 'recalé', 'appelez', 'régime', 'pleura', 'bouchon', 'vision', \"d'essayer\", 'bénite', 'travaillé', 'écoute', 'prouver', 'tombées', 'décédés', 'mecs', 'choisis', 'boire', 'coincés', 'première', 'nettoyez-le', 'bois', 'suffit', 'loin', 'aime', 'mouillée', 'trichent', 'riches', 'survivrons', 'attendu', 'homme', 'vos', 'parlerai', 'serais-je', 'givré', \"n'as-tu\", \"s'efforce\", 'avocats', 'curieuse', 'saisissez-moi', 'haleine', \"d'un\", 'ici', 'lever', 'grisant', 'malins', 'pas', \"j'étais\", 'collé', 'écarte-toi', 'chef', \"d'aucune\", \"l'utiliser\", 'dévoué', 'arrêtez', 'promets', \"l'affaire\", 'ponctuelles', 'payez-lui', 'demandez', 'mince', 'appellez', 'mourir', 'charmant', 'chats', 'montez', 'va', 'elle', 'juste', 'raison', 'peuvent', 'scié', 'annulée', 'essayons-le', 'attention', 'cinéma', 'arrivés', 'marché', 'd', 'bain', 'violent', \"t'envie\", 'mécontente', 'réaliste', 'grossière', 'talons', 'aimes', 'soupiré', \"l'amour\", 'américaine', 'câlin', 'bonbons', 'appellez-moi', 'espions', 'fonctionnera', 'démissionner', 'rompu', 'sortit', 'fesses', 'peine', 'oublie-le', 'vaincu', 'des', 'mangez', 'femme', 'ci-dessus', 'jésus', 'êtes', 'copine', 'douce', 'biscuit', 'parle-moi', 'ambitieuse', 'revenez', 'fêtes', 'enterrée', 'héros', 'attrapé', 'gèrerons', 'résiste', 'crues', 'invité', 'menottez-le', 'embrassons-nous', 'ancien', 'bourrés', 'mésaventure', 'raisonnable', 'tirerons', 'réfléchis', 'soûl', 'obstinée', 'fûmes', 'ravissant', 'donne-lui', 'possible', \"s'ennuie\", 'bizarres', 'pastiche', 'remets-le', 'foies', 'souviens-en-toi', 'grossiers', 'comporte-toi', \"qu'était-ce\", 'perplexes', 'saqué', \"l'ignorai\", \"l'achèterais\", 'distraite', 'eues', 'permission', 'changement', 'devrions', 'endurant', 'plaisez', 'strict', 'lutté', 'carabine', 'joindre', \"n'avez-vous\", 'as-tu', 'étais-tu', 'paierai', 'acquisition', 'décliner', 'gens', 'raccrocha', 'fredonne', 'mets-toi', 'peut', 'raconte', 'andouille', 'tenu', 'avance', 'grouiller', 'droite', 'dollars', 'sait', 'discrètes', 'page', 'habillez-vous', 'cd', 'disposer', 'écrasés', 'ouais', 'adultes', 'plaignez', 'couvert', 'déjà', 'obéir', 'charme', 'réveillez-vous', 'grossis', 'ferai', 'aller', 'planté', 'viendrai', 'admets', 'drôle', 'souvent', 'membre', 'empire', \"l'instinct\", 'plaqué', 'réside', 'profondeur', 'cassées', 'restée', 'fermez', 'méticuleuse', 'dimanche', 'goûts', 'vérifierai', 'sensible', 'aide-nous', 'licencié', 'demande-leur', 'douche', 'téléphoné', 'application', 'travaille', 'désolés', 'aimons', 'crus', 'mécontent', 'étions', 'discuter', 'presque', 'attache', 'sincères', 'bref', 'nouvelles', 'bougonne', 'ruinés', \"s'aggrave\", 'sa', \"l'envie\", 'élan', 'vous', 'tourné', 'laissez', 'partons', 'pensionnée', 'vannée', 'perdu', 'essaies-en', 'déprimé', \"n'abandonnez\", 'orphelin', 'mélangé', 'tôt', 'désespérez', 'veinards', 'stoppez', 'passé', 'coûte', 'stone', 'fâché', 'bourré', \"m'y\", 'pique-niques', \"l'importance\", \"n'en\", 'cultivé', 'augmentez-le', 'piochez', 'épuisé', 'vues', 'gueule', 'puissante', 'plat', 'surveille', 'bagues', 'partie', 'calme-toi', 'colère', 'tais-toi', \"l'enfer\", 'coincée', 'conduirons', 'approuve-t-on', 'inquiète', 'devant', 'abandonnèrent', 'lisez', 'brillants', 'ceux-ci', 'puis-je', 'problème', 'dodu', 'bague', 'riz', 'idée', 'non', 'emporter', 'but', 'sauvez-vous', 'rappelez-moi', 'ravi', 'partagerons', 'licite', 'embauchez-vous', 'retard', 'propre', 'anxieux', 'regarde-moi', 'allongé', 'avez-vous', 'garde', ':', 'souk', 'parie', \"j'annulerai\", 'épuisée', 'quelques-uns', 'absent', 'survis', 'grimpez', 'véhiculé', 'défaut', 'dalle', 'tuez', 'voitures', '8', 'ris', '%', 'vôtre', 'faut', 'refuse', 'coincé', 'mer', 'mensonge', 'devenu', 'marrante', 'cheval', 'allongés', 'transpiré', 'rapidement', 'droguée', 'coupe-le', 'deux', 'la', 'soucie', 'talent', 'contribué', \"m'a\", 'verrouillé', 'faire', 'marchez', 'paniquez', 'devez', 'aboient', 'étions-nous', 'simulation', 'vas-tu', 'bruyant', 'maigrichonne', 'vivrai', 'oublia', 'larmes', 'assoupie', 'asiatiques', 'parle', 'hâlée', 'sentis', 'point', 'conçue', 'discutions', 'raté', 'histoire', 'vivante', 'nombre', 'douloureux', 'superbe', 'intriguée', 'conduis', 'coulons', 'éteignez-le', 'plané', 'réparé', 'coupai', 'crocs', 'spéciaux', 'chose', 'paralysé', 'apportez', 'cancer', 'clémente', 'aérez', 'désarmée', \"j'essaye\", 'mourons', 'initié', \"l'emploi\", \"l'ai\", 'bizarre', 'vieille', 'vérité', 'quant', 'service', 'travaillons', 'gagner', 'reprises', \"n'abandonne\", 'varient', \"m'asseoir\", \"j'abandonnerai\", 'capturez-le', 'devenue', 'télé', 'montrerai', 'répare', 'laissés', 'vendu', 'lasses', 'tué', 'aîné', 'gamins', 'jeunes', 'achetons', 'dormi', 'flaire', 'payer', \"l'appartement\", 'grassouillette', 'goûtez', 'paix', 'avait-il', 'courtise', 'préoccupé', 'reculez-vous', 'ponctuels', 'bilingue', 'épinards', \"l'estomac\", 'thomas', 'attentive', 'souciez', 'bœuf', 'mourant', 'arnaqué', 'plaît', 'moi-même', 'clair', 'rationnel', 'immunisée', 'jugez', 'déplaît', 'sais-tu', 'chercher', 'humain', 'emmerdeur', 'note', 'copie', 'donnez-le-moi', 'arabes', 'auparavant', 'tiens', 'refais-le', 'échappé', 'obéirons', 'travailla', 'rendons', \"t'entends\", 'menteurs', 'dansez', 'ponctuel', 'bretelles', 'informez-en', 'sûrs', 'hanche', 'rapide', 'paye', 'intelligentes', 'stylo', 'manque', 'honorée', 'active', 'fout', \"s'enfuit\", 'joindrai', 'mouche-toi', 'échouerons', 'détesté', \"l'antipathie\", 'feu', 'soudoyés', \"l'as\", 'vilain', 'tentez-le', 'encore', 'arrivé', 'puriste', 'blaguais', 'cool', 'prête-moi', 'vivants', 'marié', 'hache', 'là-dessus', 'façons', 'faux', 'accord', 'maline', 'magnifique', 'fichier', 'déverser', 'promis', 'emploie', 'cerf', \"t'en\", 'aveugle', 'manques', 'poids', \"l'espace\", \"l'urticaire\", 'dérange', 'essaie-le', 'timides', 'lis-la-moi', \"l'écart\", 'ennemis', 'affairé', 'regarde', \"j'accepterais\", 'boisson', 'craques', 'personnel', \"j'éternue\", 'sérieux', 'arrivez-vous', 'tard', 'sushis', 'saturne', 'lundi', 'envoyée', 'flic', 'simple', 'rappelle-moi', 'baisse', 'ronfle', 'quittée', 'différente', \"t'agace\", 'habituellement', 'également', 'étoile', 'tenez-vous', 'hais', 'discret', 'dites-moi', 'confiantes', \"l'automne\", 'vasculaire', 'balle', 'muté', 'accrochez-vous', 'sommes', 'encre', \"d'herbe\", 'teint', 'réparai', 'pétoche', 'cessé', 'allons-nous-en', 'jazz', 'enlacés', \"t'ennuies-tu\", 'fortes', 'voix', 'marré', 'signifiait', 'créative', 'poupée', 'même', 'passade', 'donne-moi', 'aère', \"l'homme\", 'idiots', 'sonnez', \"t'entendre\", 'urgent', 'abandonné', \"l'éventail\", 'radotez', 'mourions', 'commencé', 'pense', \"donne-m'en\", 'suis-le', 'rêvent-ils', 'lis-le-moi', 'trichez', 'démissionné', 'mourez', 'saigner', 'chiens', 'n', 'plaisantes', 'veuillez', 'fer', 'changeons', 'cru', 'pénétrer', \"m'ennuyez\", 'noue', 'passe-moi', 'souhaits', 'ferais', 'hâlé', 'salace', 'connais-tu', \"j'attends\", 'devrais', 'amies', 'senti', 'génie', 'désire', 'nié', 'regarder', 'tracez', 'mariée', 'sommes-nous', 'inquiéter', 'recrutés', 'ah', 'désolée', 'inspirez', 'usage', 'neigé', 'étais', 'survécûmes', 'grève', 'revient', \"j'arrêterai\", 'triche', 'libère', 'minutieuse', 'coureur', 'choqué', 'grassouillet', 'égal', 'expulsée', 'montre-moi', 'plan', 'remettez-le', 'parla', 'peut-il', 'jambes', 'qui', \"d'yeux\", 'corrigée', \"l'attraperai\", 'neufs', 'grondée', 'réveillez', 'chanter', 'serai', 'travailles', 'dire', \"dis-m'en\", 'chanceuse', 'reviens', 'réveil', 'facilement', 'perdrai', \"l'été\", 'anniversaire', 'criminalité', 'insolence', '2:30', 'moyen', 'continuer', 'préparez-vous', 'disputer', 'gardes', 'étaient', 'lent', 'aide', \"d'une\", 'défaites', 'foutus', 'sur', 'dépassé', 'échoué', 'paie', 'ignorez-les', 'firefox', 'allongées', \"l'appareil\", 'fiça', 'karaoké', 'volontaire', 'pose-le', 'pouvons-nous', 'se', 'piège', 'miennes', 'plains', 'incroyable', 'certains', 'match', 'essayer', 'refusé', 'sert', 'environs', 'mordu', 'tombe', 'laissez-vous', 'écartez-vous', 'brûlée', 'pousse-toi', 'sentit', 'charrie', 'raoul', 'noir', 'idiotes', 'abandonner', 'secoué', 'toujours', 'vivre', 'embrasse-moi', 'allée', 'laissées', 'arme', 'verbalisé', 'belle', \"n'attends\", 'sera', 'tu', 'tournait', 'entendons', 'asseoir', 'sincère', 'semaine', 'radote', 'couler', 'fumes-tu', 'détestais', 'fuit', 'creusez', 'nagé', 'médecin', 'recrutée', 'contentes', 'drogué', 'petits', 'parc', 'restez', 'réveillé', 'brisée', 'saint', 'tonnes', 'reprochez', 'étudiez', \"l'avez-vous\", 'seul', 'terrifié', 'mords', 'couard', 'louve', 'ment', 'rencontrer', 'cassé', 'menteuse', 'honteuse', \"m'évite\", 'écrivez-moi', \"n'espère\", 'mémorisez-la', 'commère', \"l'air\", 'pipeau', 'plantes', 'haussé', 'sales', 'attendez-nous', 'fantastique', 'français', 'éprouve', 'santé', 'toussota', 'simplement', 'hideux', 'chic', \"l'italien\", 'reconstruirons', 'réparer', 'voulez-vous', 'pied', 'gentille', 'haussa', 'mal', 'toi-même', 'voter', 'bagages', 'collation', 'c', 'envie', \"j'emprunte\", 'chevaux', 'derrière', 'poisse', 'défoncé', 'prudence', 'fiancé', 'manquer', 'papa', \"j'essayerai\", \"c'en\", 'obèse', 'location', 'prudentes', 'romantique', 'femmes', 'nu', 'coincées', 'sors', 'américain', 'ceux-là', 'nies-tu', 'remuées', 'reconnaissante', 'siennes', 'ennuyez-vous', 'baiser', 'plantée', 'cruelle', 'portail', 'clairement', 'animaux', \"m'entends\", 'saumon', 'méchante', 'compliqué', 'rentrer', \"t'avais\", 'mignonne', \"n'ai\", 'déverrouillé', 'pigez', 'embrassé', 'vient', \"j'habite\", 'approuvent', 'épaté', 'danserons', \"l'essayer\", 'source', 'chauves', 'chantez', 'pièce', 'gamine', 'honte', 'trompé', \"l'étage\", 'cuit', 'filmer', 'sidérés', 'tracassez', 'ordres', 'prépare-toi', 'allez-vous', 'commence', 'assisterons', 'viens', 'avoir', 'ennuyions', 'amusant', 'épidémie', 'reprends', 'lequel', 'vin', \"j'arriverai\", 'rendez-moi', 'tuerai', 'seconde', 'tenais', 'bouger', 'parvenir', \"n'est-ce\", 'piges', 'œufs', \"c'est\", 'excusez-moi', 'gloussa', 'dure', 'eus', 'quelque', 'amendé', 'apportez-le-moi', 'règles', 'vérifiez', 'dvd', 'payé', 'raccroché', 'chanceux', 'fatigué', \"l'achèterai\", 'sauté', 'froid', \"qu'y\", 'mérite', 'tournez', 'blessé', 'sensé', 'saignait', 'croquez', \"t'aiderai\", 'soulagement', 'libre', 'écris-moi', 'détenu', 'dressé', 'boston', 'allez', 'nom', 'dérangé', 'bourrées', 'trahie', 'précipite', 'prime', 'séparons-nous', 'suivez', 'maniez-vous', 'soit-il', 'aidez-nous', 'pouvons', 'attristé', 'taxi', 'genre', 'apporte-le', 'deviens', \"l'hiver\", 'ralentissez', \"j'utilise\", 'louché', 'peux-tu', 'contredanse', 'sidérées', 'fâchés', 'savions', 'hé', 'monte-le', 'guerre', \"l'aimons\", 'prédit', 'manquez', 'consciencieuse', 'aux', 'moqué', 'fiasco', 'connaissez-vous', 'sœurs', \"l'oiseau\", 'tombés', 'signé', \"n'attendez\", 'clé', 'rebelle', 'contrôler', 'prêté', 'rappelle', 'paiera', 'lâchez-moi', 'gagneurs', 'péter', 'sont-ils', 'meilleure', 'meure', 'vois', \"t'inquiète\", 'gâteaux', 'cacher', 'fera', 'vivra-t-il', 'miens', 'sciée', 'marrée', 'élaboré', 'allumé', 'schcoumoune', 'fanatique', 'étudier', 'remercie', 'endommagé', 'affamées', \"d'enfants\", 'émettre', 'mordit', 'jeu', 'vérifié', 'équitable', 'lampe', 'masque', 'cet', \"t'es-tu\", 'confessé', 'impoli', 'hypothèse', 'appellerai', 'jumeaux', 'effrayant', 'excursions', 'enfourche', \"j'essaie\", 'déplace', \"d'espace\", 'plaisent', 'démissionna', 'trouvées', \"l'ai-je\", 'venez-vous', 'gourmand', 'mienne', 'devons', 'oublions', 'soudoyées', 'fruit', 'trente', 'survécu', 'écoutons', 'sourcils', 'stupide', 'affamé', 'il', 'sexy', 'manquant', 'vont', 'avoué', 'puis', 'démonte-moi', 'cédé', 'bateau', 'fin', 'foules', 'faim', \"j'en\", 'mercis', 'détendez-vous', 'devint', 'fermier', 'cochons', 'motivée', 'banane', 'frousse', \"l'apprécia\", 'chine', 'horreur', 'reposée', 'liberté', 'laissez-la', 'heureux', 'sommeil', 'venir', 'hésita', 'ira-t-il', \"d'air\", 'impitoyable', 'ils', 'commencerons', 'gentils', 'produisent', 'voulait', 'hasarde', 'canadienne', 'claire', 'secrets', 'forcez', 'êtes-vous', 'venue', 'montre-toi', 'montée', 'complètement', 'touchés', 'trouvèrent', 'quittes', 'espoir', 'surfeur', 'saufs', 'opposé', 'sonne', 'bons', 'pièces', 'calculé', 'rationnelle', 'regarde-le', 'heures', 'appelle-moi', 'lève', 'cuisiner', 'ligne', 'courez', \"l'aise\", 'fâchée', 'doté', 'toute', 'garçon', 'file-le', 'mentir', 'dernier', \"l'extérieur\", 'habille-toi', 'débarrassez-vous', 'chez-moi', 'recule', 'aimée', 'suis', 'assis', 'fainéante', \"s'est\", 'rendre', 'fumes', 'bougé', 'faits', 'dépêchez-vous', 'geste', 'courrier', 'ferons', 'rire', 'rencontrai', 'flemmard', 'précipitez', 'veine', 'naïve', 'attends-nous', 'pleures-tu', 'présenté', 'soutien', 'dormez', 'plaisir', 'soulever', 'bicyclette', 'garde-le', 'refaites-le', 'crois', 'suite', 'occupés', 'simule', 'équipage', 'courir', 'guéri', 'tête', 'avons', 'tente', 'passionnant', 'fais-le', 'diable', 'apprécia', 'valise', 'rendu', 'sauvé', 'chaise', 'nourri', 'cou', 'piégée', \"n'ai-je\", 'quereller', 'football', \"j'achèterai\", 'haïssait', 'embauché', 'chapeau', 'tant', 'descendit', \"m'ignora\", 'construite', 'cri', 'faites-moi', 'oh', 'coupé', 'veut', 'quelle', 'roule', 'allongée', 'selon', 'brûlées', 'réponses', 'embrassés', 'projets', 'agriculteur', 'défense', 'endormie', 'bateaux', \"l'arrêtez\", \"j'entendis\", 'larguée', 'nagera', 'agissez', 'minces', 'célèbre', 'laisse-moi', \"s'agit-il\", 'groupe', 'malchanceux', 'dressa', 'abrutie', \"l'encre\", 'tue', 'immobile', 'appelle', 'affreusement', 'russie', 'planqué', 'café', 'besoin', 'mannequin', 'très', \"s'agissait-il\", 'ville', 'asseyez-vous', 'éteins-le', 'pété', \"j'obéirai\", 'gavé', 'ménage', 'droits', 'sashimi', 'regarderai', 'reconnaissant', 'donna', 'tigres', 'ans', 'déprimée', 'montre-le-nous', 'dégueulassé', 'malheureuse', 'tentez', 'poète', 'sois', 'certain', 'connais', 'piégé', 'courageuses', 'dos', \"d'abord\", 'joueur', \"l'anglais\", 'liste', 'désarmé', 'touriste', 'croisés', 'chat', 'nerveux', 'jumeau', \"j'adorais\", 'moine', 'dépêcherai', 'est-elle', 'ingénu', 'célèbres', 'désespéré', 'juge', 'divorcé', 'sympa', 'demandez-leur', 'ronfler', 'mangera', 'cléments', \"j'enseigne\", 'inutiles', 'écouté', 'réveillée', 'surfeuse', 'tchin-tchin', 'respectueuse', 'osez-vous', 'naïf', \"l'odeur\", 'paris', 'respectueuses', 'fume', \"m'excuser\", 'était-ce', 'goberais', 'de', 'calme', 'pleuré', 'tente-le', 'trompée', 'recyclage', 'avc', 'rattrape-le', 'capté', 'mentez', 'chocottes', 'sage', 'excuses', 'fiable', \"d'identité\", 'naze', 'surprise', 'scelle', 'pois', 'trouver', 'ski', \"c'était\", 'pause', 'bétail', 'perdit', 'savez', 'heurté', 'marrées', 'davantage', \"l'apprécie\", 'cassent', 'balayer', 'connaissons-nous', \"l'art\", '(', 'voyez', 'fiancée', 'appelé', 'conseils', 'terrifiée', 'laisse-le', 'gourmande', 'crève', 'années', 'rends-toi', 'patiner', 'silence', 'aidé', 'mortes', 'expert', 'esseulé', 'golf', 'étendu', 'endurante', 'exagéré', 'insectes', 'inclus', 'musulman', 'seules', \"d'écrire\", 'insister', 'chien', \"n'importe\", 'remise', 'grosse', \"l'exercice\", 'pâle', 'téléphonai', 'bénie', 'jogging', 'trouve', 'vrai', 'voyons', 'apprécié', 'dû', 'jalouse', 'retenez', 'montrez-le-moi', 'quoi', 'faites', 'confiant', 'courus', 'avocat', 'déjeuner', 'prosélytes', 'pleurent', 'énigmes', 'armée', 'fallut', 'conduisez', 'maintenant', 'récits', 'indigné', \"l'école\", 'ma', 'aiment', 'calmes', 'londres', 'filles', 'donnez-la-moi', 'volant', 'acquérir', 'durs', 'parié', 'grande', 'trouvé', 'part', 'adopté', 'écrasées', 'maison', 'joué', 'souri', 'apporterons', 'pêcher', \"t'a-t-on\", 'entrez', 'chanteur', 'commences', 'enlacée', 'importe', 'nourris', 'poche', 'semble', 'attendrons', 'gaules', 'impossible', 'je', 'suisse', 'indignée', 'réparez', 'honnête', 'courage', 'vends', 'mariés', 'règle', 'nostalgie', 'souvenez-vous-en', 'détient', 'devrais-je', 'ennuyons', 'aidez-moi', 'injuste', \"n'est-elle\", 'obéis', 'casser', 'plaie', 'r', 'dieu', 'gardez', 'sauvez', 'méprise', 'trébuché', 'helvète', 'agréable', \"m'assis\", 'ou', 'bas', 'embête', 'vidéo', 'rendue', 'tiens-toi', 'mauvais', 'oublie', 'chauve-souris', 'ramasse-le', 'offensée', 'rigole', 'attrape-le', 'perds', 'maman', 'mettons-nous', 'chiot', 'travaillez', 'hésité', 'blague', 'une', 'tocade', 'bâtie', 'parodie', 'paniqué', 'cette', 'savait', 'fusil', ')', 'expliquerons', 'largué', \"s'évanouira\", 't', 'eûmes', 'excentrique', 'sainte', 'essayerons', 'voit', 'jamais', 'dossier', 'flingue', \"d'équerre\", 'enfants', 'cherche-le', 'reposes', 'rumeur', 'descends', 'aidez', 'retraité', 'pommes', 'demeure', \"l'emportèrent\", 'pioche', \"l'enlaça\", 'normal', 'passe-le-moi', 'semblez', 'cloué', 'poli', 'ouvre', 'fumée', 'taquine', 'conduit', 'musulmane', 'changer', 'laisse-la', 'auteur', 'devrait', 'videz', 'lirai', 'sifflé', 'regard', 'tins', 'char', 'choquées', 'embrasse', 'engagé', 'voiture', 'étourdi', 'capte', 'impolie', 'basse', \"jusqu'à\", \"j'étudie\", 'ton', 'cuisez', 'mangé', 'moitié', 'cuillère', 'souci', 'a-t-on', 'mort', 'allé', 'reine', 'examen', 'fatigués', 'laissé', \"l'arme\", 'voulions', 'laissée', 'vulgaire', \"j'écris\", 'annulé', 'peu', 'douté', 'brève', 'prévenu', 'celles-ci', 'pincé', 'tristesse', 'bonne', 'neige', 'paresseuse', 'pouvez-vous', \"t'a\", 'espionnes', \"m'aimes\", 'bronze', 'discrète', \"t'aimions\", 'plait', 'employez', 'nôtre', 'place', 'rencontrées', 'arnaque', 'crevées', 'fracassé', 'touille', 'mis', 'séparés', 'soirée', 'celles-là', 'échouer', 'inoffensif', 'chanson', 'savent', 'taille', 'parole', 'tenté', 'avant', 'ce', 'préjugés', 'prenez-le', 'nouveau', 'mentent', 'appelez-nous', 'quitté', 'discutons', 'était', 'retiens', 'visage', 'voulons', \"j'arrive\", '&', 'bienvenu', 'contact', 'remué', \"l'aime\", 'séparées', 'prudente', 'aider', 'hocha', 'sucré', 'honteux', 'téléphone', 'touchez', 'parviens', 'aide-moi', 'pardon', 'embêtant', 'pigé', 'consciencieux', 'croissent', \"m'entendez-vous\", 'blindé', 'viendra-t-elle', 'sortez', 'histoires', 'éveillé', 'appelées', 'rencard', 'choquée', 'souvenons', 'réponds', 'rendez-vous', 'tentée', 'détestes', 'huit', 'paie-moi', 'nouez', 'heureuses', 'saoule', 'pleurent-ils', 'joyeux', \"n'avais\", 'discrets', 'pigez-vous', \"l'emporter\", 'sidéré', 'vaut', 'lèvres', 'four', 'sers-toi', 'malchanceuses', 'mens', 'mouvement', 'préparé', 'désespère', 'affamés', \"l'emportâmes\", 'brûlé', 'cours', 'crevée', 'dessus', 'riez', 'connaissons', 'véhicule', 'sentez-vous', 'exclu', 'engagée', 'protesté', 'trouvez', 'amène', 'envoyé', 'cassés', 'laisse-nous', 'fidèle', 'celui-ci', 'trépasser', 'admirateurs', 'sienne', 'têtue', 'emploi', 'laissons', 'focaliser', 'amis', 'intention', 'fringale', 'confiants', 'patiente', 'profite', 'fait', 'haletait', 'passe-t-il', 'impressionnée', 'amicaux', 'affaires', 'marchera', 'conclu', 'fait-il', \"t'aider\", 'décoré', 'serrez-moi', 'certaines', 'effarant', 'gagne', 'comporte', 'perdîmes', 'partirai', 'temps', 'gaz', \"d'arriver\", 'ingénue', 'fauché', 'courageux', 'poste', 'passe', 'mourras', 'morts', 'sac', 'gronda', 'relâchez-le', 'contrôle', 'attentivement', 'souriez', 'chameaux', 'écouta', 'décalé', 'amicale', 'poussez', '.', 'fume-t-il', 'échauffer', 'pieds', 'toutes', 'toubib', 'vole', 'renaître', 'mode', 'rusés', 'jeta', 'bayer', 'opération', 'allons', 'soit', 'mauvaise', 'nage', 'nagèrent', 'fâchées', 'abandonnez', \"j'aie\", \"d'idiot\", 'y', 'givrés', 'bu', 'discutâmes', \"qu'est\", 'offrez-moi', 'malédiction', 'manquait', '\\t', 'travail', 'sentie', 'oubliez', 'vert', 'attendîmes', 'parages', 'dépêche-toi', 'nuageux', 'ragote', 'attendra', 'émue', 'manque-t-il', 'ceci', 'sauvés', 'sauve', 'comment', 'grossières', 'moindre', 'courte', 'pépère', 'nourrissent', 'ressentir', 'céda', 'lavé', 'sèche', 'attaque', 'assurée', 'marcher', 'réparée', 'doucement', 'héroïque', 'voulais', 'écrivain', 'rouler', 'villes', \"l'espoir\", 'chatouilleuse', 'mettrai', 'monde', 'comprends', 'repose-toi', 'ferais-je', 'regardez-moi', 'dans', 'charlatan', 'levées', 'voté', 'française', 'kobe', 'manger', 'battues', 'langue', 'savoir', 'pris', 'crains', 'préséance', 'froids', 'sable', 'crie', 'goûte', 'pays', 'tokyo', 'plaisante', 'libres', 'distrait', 'court', 'giflé', 'rentrée', 'sûres', 'fête', 'monte', 'besoins', \"l'opéra\", 'foiré', 'bruit', 'inutile', 'oppose', 'lâche-moi', 'grouille', 'tomber', \"j'attendrai\", 'patient', 'entretînmes', 'affûtée', 'légal', \"l'appellerai\", 'adorerais', 'bientôt', 'âge', 'abruti', 'enceinte', \"m'arrange\", 'fatiguée', 'sympas', 'minutieux', 'mug', 'connues', 'plancher', \"j'allais\", 'veinarde', 'montez-le', 'pars', 'montre', 'meurt', 'cuis', 'renard', 'fus', 'enseignante', 'bol', 'blesse', 'snob', 'venu', 'laissez-moi', 'fini', 'loucha', 'dépêcher', 'perdis', 'bénisse', 'été', 'avions', 'apeurée', 'gentil', 'ai-je', 'foutre', \"j'élève\", 'briser', 'fauchées', \"qu'il\", 'irons', 'allumée', 'goûtez-la', 'crédule', 'pauvre', 'impôts', 'amène-moi', 'échappées', 'touchées', 'veinard', 'vanter', 'sinistres', 'plans', 'train', 'ignore', 'reculez', 'tortues', 'plaquée', '100', 'maths', 'perdues', 'mythe', 'synchronisées', 'va-t-il', 'aille', 'innocente', 'froides', 'intrépide', 'démarré', 'cruels', 'évident', 'finissez', 'étudie', \"l'avais\", 'vivantes', 'demanderai', 'jumelles', 'triché', 'cache', 'incapable', 'ravie', 'démarrez', 'colle', 'change', \"j'arrête\", 'affaire', \"s'y\", 'jolie', 'muffins', 'créatif', 'vus', 'mémorisez-le', 'jouez', 'crevé', 'doutes', 'retirez-vous', 'essuie', 'perte', 'debout', 'pâli', 'énervée', 'pleurerai', 'connaît', 'grave', 'verbalisée', 'ponctuelle', 'faillite', 'failli', 'nuit', 'miséricordieuses', \"m'appelle\", 'charriez', 'fou', 'bats', 'là-bas', 'agis', \"d'imbécile\", 'pratiques-tu', 'ranch', 'lycée', 'trois', 'cesse', 'nous', 'pleurez-vous', \"d'elles\", 'important', 'encombrant', 'sec', 'ami', 'fut', 'pétrin', 'désormais', 'tiré', \"l'emporte\", 'observatrice', 'sentent', 'offensé', 'plante', 'sûr', 'souhait', 'verre', 'battez-vous', 'futon', 'compatis', 'est-ce', 'affreux', 'chaud', 'pardonne-nous', 'utilise', 'faibles', 'battus', 'bonjour', 'enlacées', 'malheur', 'évanouie', 'sirènes', 'cirez', 'malpoli', 'japonais', 'recul', 'camp', 'vint', 'repasser', 'endormi', 'vis', 'flics', 'merdé', 'intelligent', 'sauvée', 'emporté', 'dispose', 'déchaîné', 'yeux', 'gagnerons', 'venez', 'timide', 'rentre', 'regarda', 'adulte', 'oublié', 'bénéfique', 'chauffeur', 'rendrez-vous', ',', 'souffre', 'amusez-vous', 'impliqué', 'sinistre', 'on', 'main', 'réveillés', 'glace', 'mémorise-la', 'nourrissez', 'recrutez-vous', 'neuves', 'panique', 'réécrit', 'pleine', 'quel', 'oubliez-le', 'conduire', 'tranchante', 'comme', \"m'ennuies\", 'adore', 'joues', 'affolée', 'lion', 'crié', 'fermés', 'lisez-le-moi', 'essaie', 'coffre', 'saisirai', 'pousse', 'trou', 'culot', \"m'aimez-vous\", \"d'accord\", 'lit', 'obtenu', 'brûle', 'réécrivis', 'humble', 'droit', 'continue', 'connaissent', 'rouge', 'raisonnables', 'dix-huit', 'étiez', \"l'emporterons\", 'tiendrons', 'permettez-moi', 'naïves', 'chatouilleux', 'construit', 'seuls', 'prêtre', 'tremper', 'taudis', 'chemin', 'forts', 'coup', 'prie', 'levée', 'chaussures', 'plaisanterie', 'ironique', 'cassée', 'buvais', 'signerai', 'cocufiée', 'jubile', 'lignes', 'suivez-nous', 'rusé', 'illuminé', 'passée', 'foutu', 'titulaire', 'furieux', 'détritus', 'garçons', 'quelconques', 'voyages', 'dure-t-il', 'as', 'rit', 'apprécions', 'assuétude', 'apporté', 'matériel', 'ambitieux', 'blessés', 'mouches', 'prise', \"m'aime\", 'conduirai', 'paresseuses', 'menteur', \"l'oubliez\", 'génial', 'corrigé', 'a', 'coucher', 'marche', 'attendez', 'voici', 'perdre', 'resterai', 'rentrez', 'toupet', 'continuerai', 'levés', 'cercle', 'obéissez', 'verrouille', 'entretenus', 'célébrons', \"conte-m'en\", 'expliquer', 'faible', 'mesquine', 'remuée', 'écoulé', 'évanoui', \"s'est-il\", 'arrêtez-les', 'font', 'sûre', 'avion', 'pour', 'hâter', 'travaillais', 'paraît', 'sorties', \"l'argent\", 'faites-le', 'pot', 'béni', 'marque', 'loyal', 'blessée', 'expulsé', \"n'étais\", \"d'elle\", 'hommes', 'gentilles', 'dépêcherons', 'indice', 'travaillerai', 'patent', 'leva', 'grands', 'marrant', 'sang-froid', 'gavés', 'plus', 'horrible', 'neiger', 'peau', 'me', 'givrées', 'agir', 'pleuvoir', 'fleurs', \"j'ai\", \"j'envie\", \"l'étreignis\", 'amicales', 'calmez-vous', 'embrasser', 'passera-t-il', 'refusa', 'test', 'bagnole', 'tombée', 'ensemble', 'décorée', 'partez', 'occupé', 'gloussé', 'tom', 'pue', 'adorait', 'contents', 'enfourchez', 'avais', 'faveur', 'décontracté', 'fiez-vous', 'rends', 'retraitée', 'amuse-toi', 'ira', 'recrutées', 'gaffe', 'essayons-la', 'dépasse', 'volent', 'reposer', \"n'avons\", 'dîner', 'arriverons', 'plaît-il', 'idiote', 'films', \"qu'est-ce\", 'préoccupée', 'rassasié', 'ordonné', 'savons', 'piégés', 'boulot', 'largeur', \"m'insère\", 'sentez', 'entendu', 'bois-tu', 'prêts', 'tarte', 'leurs', 'armés', 'inspire', 'faisons-le', \"m'en\", \"m'apprécie\", 'mieux', 'fumez-vous', 'danseuse', 'surviendra', 'propres', 'disparais', 'prévenues', 'rend', 'merci', 'buvez-le', 'tirer', 'chanta', \"l'heure\", 'mignonnes', 'fiche', 'mortel', 'conditionnelle', 'cuisinerai', 'retraite', 'timbre', 'rusée', 'a-t-il', 'pardonnez-moi', 'battrai', 'goûtez-le', 'sauta', 'perdue', \"j'appellerai\", \"l'eau\", 'râler', 'grognon', 'retour', 'gobe', 'exact', 'approche', 'faisons', 'confectionné', 'détestait', 'tenez', \"l'abri\", \"l'a-t-il\", 'fumez', 'cesser', 'que', 'lâchez-vous', 'ennuyeux', 'occupées', \"l'arrêt\", 'prouverai', 'sorti', 'lèverai', 'donne-la-moi', 'tirées', 'corneilles', 'arbre', \"t'aimais\", \"l'arabe\", 'suivies', 'te', 'assoupi', 'moi', 'avertis', \"n'était\", 'couper', 'crevés', 'excitant', 'sérieusement', 'désespérée', 'travaillerons', 'attrapés', 'protester', 'cuisine', 'beaucoup', 'réussirons', 'rhume', 'hurlé', 'laissa', 'appellera', 'magnez-vous', 'égoïstes', 'si', 'tracasse', 'souris', 'dépêchons-nous', \"j'avais\", 'tua', 'grossier', 'tombé', 'route', 'gosse', 'fis', 'sauves', 'goûte-la', 'embauches-tu', 'alité', 'grandi', 'nerveuse', 'augmentation', 'brûler', 'carte', 'essayez', 'innocent', 'arriver', 'joignez-vous', 'nos', 'invitée', 'vol', 'prends-en', 'beauté', 'bordel', 'pleurait', 'cous', 'bière', 'boulangère', \"m'est\", 'raisin', 'cligné', 'cloche', 'questions', 'rends-moi', 'contrarié', 'prends-toi', 'buvez-vous', 'intentions', 'formez', 'tous', \"j'y\", 'servante', 'parties', 'lave-toi', 'souffrait', 'veillez', 'vu', 'aucun', 'souhaite-moi', 'plate', 'au-dessus', 'répondez', 'pourrais-je', 'cela', 'trompe', 'obsolète', 'fié', 'parlez-moi', \"t'inquiéter\", 'jouer', 'es-tu', 'ennuyez', 'connus', 'avides', 'finis', 'refuser', 'là-haut', 'appelés', 'tire', 'proximité', 'rigolo', 'poursuivrai', 'rock', 'e', 'présente', 'malandrin', 'confiante', 'ongle', 'payez-moi', 'pleurer', 'remplace-moi', 'revoir', 'coréen', 'régimes', 'doute', 'précipité', \"d'outils\", 'partial', 'répondre', 'type', 'chouette', 'arrive', 'quittez', 'dressai', 'poésie', 'galant', 'étiez-vous', 'homard', 'dois', 'ventilateur', 'partir', 'envoyez-le', 'attachez', 'séparâmes', 'échecs', 'pardi', 'remonté', 'tirés', 'accro', 'écrasé', 'écrivez', 'logique', 'nausée', 'devoir', 'remarqué', 'ressenti', 'entré', 'démissionnons', 'redresse-toi', 'danser', 'gros', 'saisissez-vous', 'hors', 'apprécie', 'attendre', 'livres', 'véritable', \"l'ail\", 'assuré', \"l'arrête\", 'tousse', 'couvrez-le', 'amusante', 'fréquemment', 'accompli', 'avait', 'entra', 'levé', 'trop', 'vérifierons', 'avide', 'soupe', 'lisez-la-moi', 'frimer', 'lance', \"j'éprouve\", 'improvisé', 'mentit', 'déménager', 'stricte', 'disparu', 'dernière', 'toucher', 'fois', 'aura', 'poison', 'tranquille', 'miséricordieux', 'vexé', 'chanteuse', 'patine', 'doit-il', 'porte', 'voulez', 'perplexe', 'dépêché', 'juré', 'matin', 'oncle', 'vite', 'miel', 'avez', 'montrez', 'huîtres', 'dingue', 'niez', 'fort', 'passa', \"t'ai\", 'relâche-le', 'pourrais', 'mignons', 'poursuivons', 'insisté', 'filez', 'ruinées', 'plaira', 'fais-lui', 'viré', 'moment', \"l'ont\", 'petite', \"d'aller\", 'sombre', 'lèvre', 'montrez-la-moi', 'flan', 'laquelle', 'essayez-en', 'accident', 'tournée', 'décide', '$', 'vas-y', 'taille-toi', 'coups', 'fourbu', 'noyé', 'voyez-vous', 'châteaux', 'fauchés'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CuQ7B78BhLrx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Task 3 (10 marks)**\n",
        "\n",
        "---\n",
        "\n",
        "*  Assign each unique word to an integer value (5 marks).\n",
        "*  Create word embedding for your vocabulary using pretrained Glove embeddigns (5 marks) (http://nlp.stanford.edu/data/glove.6B.zip) [see Lab 7]\n",
        "* Print the first line of the embeddings (see below) "
      ]
    },
    {
      "metadata": {
        "id": "pPq-c1tqfoRO",
        "colab_type": "code",
        "outputId": "8ba59001-a02e-4f79-8729-33a72bd377f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#your code goes here\n",
        "import numpy as np\n",
        "dic_targetVol={}\n",
        "dic_sourceVol={}\n",
        "#create the dictionary of source vocabulary and target vocabulary, the key are sentece and the value are index \n",
        "dic_sourceVol = dict([(char,i) for i, char in enumerate(Source_vocabulary)])\n",
        "dic_targetVol= dict([(char,i) for i, char in enumerate(target_vocabulary)])\n",
        "print(len(dic_sourceVol))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I7G4tGZwgVUW",
        "colab_type": "code",
        "outputId": "9dad64dc-543a-4d7d-d6cd-a34af3833515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "#create the embedings for each sentence, here I use glove.6B.50d, for each line get the \n",
        "#word and embeding features for this word\n",
        "def read_data(file_name):\n",
        "    with open(file_name,'r') as f:\n",
        "        word_vocab = set() \n",
        "        word2vector = {}\n",
        "        for line in f:\n",
        "            line_ = line.strip() \n",
        "            words_Vec = line_.split()\n",
        "            word_vocab.add(words_Vec[0])\n",
        "            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n",
        "    print(\"Total Words in DataSet:\",len(word_vocab))\n",
        "    return word_vocab,word2vector\n",
        "#the vocab is all words in this file and the w2v is a diactionary, the key is each word and\n",
        "#the vlaue is feature embeding for each word.then firstly check whether this word in the vocab,\n",
        "#then get the embeddings of this word \n",
        "def get_embedings(dic_sourceVol):\n",
        "  vocab, w2v =read_data(\"/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt\")\n",
        "  print(w2v['good'])\n",
        "  embeddings_source={}\n",
        "  for val,key in dic_sourceVol.items():\n",
        "      if val in vocab:\n",
        "        embeddings_source[key]=w2v[val]\n",
        "      else:\n",
        "        embeddings_source[key]=np.zeros(len(embeddings_source[0]),dtype='float32')\n",
        " \n",
        "  return embeddings_source\n",
        "\n",
        "embeddings_source=get_embedings(dic_sourceVol) \n",
        "#this is the first embeding\n",
        "# print(embeddings_source[0])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Words in DataSet: 400000\n",
            "[-3.5586e-01  5.2130e-01 -6.1070e-01 -3.0131e-01  9.4862e-01 -3.1539e-01\n",
            " -5.9831e-01  1.2188e-01 -3.1943e-02  5.5695e-01 -1.0621e-01  6.3399e-01\n",
            " -4.7340e-01 -7.5895e-02  3.8247e-01  8.1569e-02  8.2214e-01  2.2220e-01\n",
            " -8.3764e-03 -7.6620e-01 -5.6253e-01  6.1759e-01  2.0292e-01 -4.8598e-02\n",
            "  8.7815e-01 -1.6549e+00 -7.7418e-01  1.5435e-01  9.4823e-01 -3.9520e-01\n",
            "  3.7302e+00  8.2855e-01 -1.4104e-01  1.6395e-02  2.1115e-01 -3.6085e-02\n",
            " -1.5587e-01  8.6583e-01  2.6309e-01 -7.1015e-01 -3.6770e-02  1.8282e-03\n",
            " -1.7704e-01  2.7032e-01  1.1026e-01  1.4133e-01 -5.7322e-02  2.7207e-01\n",
            "  3.1305e-01  9.2771e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2QfLgKEgazro",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 2 Translation Model training\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Y8WnlX8d0RVj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Task 3 (20 marks)**\n",
        "* Provide code for the encoder using Keras LSTM (5 marks)\n",
        "* Provide code for the decoder using Keras LSTM (5 marks)\n",
        "* Train the sequence2sequence (encoder-decoder) model (10 marks)\n"
      ]
    },
    {
      "metadata": {
        "id": "F_sLH38o0BJq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# encoder code goes here\n",
        "# import numpy as np\n",
        "#for the encoder and decoder, here firsly I use the embeding features as encoder, the generation way is \n",
        "# define the encoder input data shape. the length is smaple numers, for each sample the length is the max encoder\n",
        "#sequence length and for each word the length is embeding feature length. The according to each words in the input text to\n",
        "#get their embeding features, rest of length filled by zero.\n",
        "def get_encoder_input_data(input_texts,max_encoder_seq_length,embeddings_source):\n",
        "    encoder_input_data=np.zeros((len(input_texts),max_encoder_seq_length,len(embeddings_source[0])),dtype='float32')\n",
        "    for sen in range(len(input_texts)):\n",
        "        source_tokenize1=input_texts[sen]\n",
        "        for words in range(len(source_tokenize1)):\n",
        "                  for i in range(len(embeddings_source[0])):\n",
        "                      encoder_input_data[sen,words,i]=embeddings_source[dic_sourceVol[source_tokenize1[words].lower()]][i]\n",
        "                      # embeddings_source[dic_sourceVol[source_tokenize1[word].lower()]][i]\n",
        "    return encoder_input_data    \n",
        "# for i,input_text in enumerate(input_texts):\n",
        "#   for t, char in enumerate(input_text):\n",
        "#       encoder_input_data[i,t,dic_sourceVol[char.lower()]]=1    \n",
        "# print(encoder_input_data[0])\n",
        "encoder_input_data=get_encoder_input_data(input_texts,max_encoder_seq_length,embeddings_source)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "erw3s9vI0scb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# decoder code goes here\n",
        "# for the decoder data, here I use the one-hot method to get the decoder data, the decoder data has two part, first is input data, second is target data\n",
        "# the target data is the next word of input data, for each word using one-hot to get the feature vector as input\n",
        "def get_decoder_data(target_texts,max_decoder_seq_length,num_decoder_tokens):\n",
        "    decoder_input_data=np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
        "    decoder_target_data=np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
        "    for i,target_text in enumerate(target_texts):\n",
        "      for t, char in enumerate(target_text):\n",
        "          decoder_input_data[i,t,dic_targetVol[char.lower()]]=1\n",
        "          if t>0:\n",
        "            decoder_target_data[i,t-1,dic_targetVol[char.lower()]]=1\n",
        "    return decoder_input_data,decoder_target_data\n",
        "  \n",
        "decoder_input_data,decoder_target_data=get_decoder_data(target_texts,max_decoder_seq_length,num_decoder_tokens)\n",
        "#shape of the decoder target data\n",
        "print(decoder_target_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xe3hV0b00uJN",
        "colab_type": "code",
        "outputId": "b78c6223-0545-4746-9107-1b61a58d039c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "#this is seq2seq network,the input is embedings of encoder data,add a LSTM layer to network\n",
        "#then get the state, for hidden state and cell state, then using the hidden state and cell state as \n",
        "#the context and new input of decoder\n",
        "batch_size=64\n",
        "epochs= 30\n",
        "latent_dim =256\n",
        "encoder_inputs=Input(shape=(None,len(embeddings_source[0])))\n",
        "encoder_lstm= LSTM(latent_dim,return_state=True)\n",
        "encoder_outputs,state_h,state_c= encoder_lstm(encoder_inputs)\n",
        "encoder_states= [state_h,state_c]\n",
        "\n",
        "#the input is one-hot, the network structure is same with encoder, add input layer, one LSTM layer and \n",
        "#one dense layer, feed the input decoder data and encoder states.\n",
        "decoder_inputs=Input(shape=(None,num_decoder_tokens))\n",
        "decoder_lstm=LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_output,_,_=decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
        "decoder_dense= Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs=decoder_dense(decoder_output)\n",
        "\n",
        "model= Model(inputs=[encoder_inputs,decoder_inputs],outputs=decoder_outputs)\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           (None, None, 50)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           (None, None, 3948)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  [(None, 256), (None, 314368      input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_12 (LSTM)                  [(None, None, 256),  4305920     input_20[0][0]                   \n",
            "                                                                 lstm_11[0][1]                    \n",
            "                                                                 lstm_11[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, None, 3948)   1014636     lstm_12[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,634,924\n",
            "Trainable params: 5,634,924\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9AibUk5I0UWX",
        "colab_type": "code",
        "outputId": "058ac5b7-c0fd-4ec0-926c-a6e6f788932b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "#generate the train and test dataset, here I using train_test_split to split it, the split size is 0.2, the output below is shape of training and testing data\n",
        "X_train_encoder_input_data,X_test_encoder_input_data,y_train_decoder_target_data,y_test_decoder_target_data=train_test_split(encoder_input_data,decoder_target_data,test_size=0.2,random_state=0)\n",
        "X_train_decoder_input_data,X_test_decoder_input_data=train_test_split(decoder_input_data,test_size=0.2,random_state=0)\n",
        "train_input_text,test_input_text,train_target_text,test_target_text=train_test_split(input_texts,target_texts,test_size=0.2,random_state=0)\n",
        "print(X_train_encoder_input_data.shape)\n",
        "print(X_test_encoder_input_data.shape)\n",
        "print(X_train_decoder_input_data.shape)\n",
        "print(X_test_decoder_input_data.shape)\n",
        "print(y_train_decoder_target_data.shape)\n",
        "print(y_test_decoder_target_data.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6400, 6, 50)\n",
            "(1600, 6, 50)\n",
            "(6400, 13, 3948)\n",
            "(1600, 13, 3948)\n",
            "(6400, 13, 3948)\n",
            "(1600, 13, 3948)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ovWQs8KG09we",
        "colab_type": "code",
        "outputId": "80b8ce6d-3cc6-45c9-c5ea-0ec90219b641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1108
        }
      },
      "cell_type": "code",
      "source": [
        "#fitting data and saving data.\n",
        "model.fit([X_train_encoder_input_data, X_train_decoder_input_data], y_train_decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "model.save('seq2seq_source_target.h5')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5120 samples, validate on 1280 samples\n",
            "Epoch 1/30\n",
            "5120/5120 [==============================] - 7s 1ms/step - loss: 1.9069 - val_loss: 1.7652\n",
            "Epoch 2/30\n",
            "5120/5120 [==============================] - 4s 828us/step - loss: 1.6183 - val_loss: 1.6456\n",
            "Epoch 3/30\n",
            "5120/5120 [==============================] - 4s 824us/step - loss: 1.4517 - val_loss: 1.5099\n",
            "Epoch 4/30\n",
            "5120/5120 [==============================] - 4s 832us/step - loss: 1.3070 - val_loss: 1.4147\n",
            "Epoch 5/30\n",
            "5120/5120 [==============================] - 4s 830us/step - loss: 1.2036 - val_loss: 1.3643\n",
            "Epoch 6/30\n",
            "5120/5120 [==============================] - 4s 830us/step - loss: 1.1263 - val_loss: 1.3164\n",
            "Epoch 7/30\n",
            "5120/5120 [==============================] - 4s 854us/step - loss: 1.0619 - val_loss: 1.2703\n",
            "Epoch 8/30\n",
            "5120/5120 [==============================] - 4s 874us/step - loss: 1.0060 - val_loss: 1.2405\n",
            "Epoch 9/30\n",
            "5120/5120 [==============================] - 4s 851us/step - loss: 0.9536 - val_loss: 1.2189\n",
            "Epoch 10/30\n",
            "5120/5120 [==============================] - 4s 828us/step - loss: 0.9043 - val_loss: 1.1985\n",
            "Epoch 11/30\n",
            "5120/5120 [==============================] - 4s 821us/step - loss: 0.8598 - val_loss: 1.1869\n",
            "Epoch 12/30\n",
            "5120/5120 [==============================] - 4s 836us/step - loss: 0.8154 - val_loss: 1.1629\n",
            "Epoch 13/30\n",
            "5120/5120 [==============================] - 4s 831us/step - loss: 0.7762 - val_loss: 1.1457\n",
            "Epoch 14/30\n",
            "5120/5120 [==============================] - 5s 897us/step - loss: 0.7382 - val_loss: 1.1377\n",
            "Epoch 15/30\n",
            "5120/5120 [==============================] - 5s 896us/step - loss: 0.7029 - val_loss: 1.1178\n",
            "Epoch 16/30\n",
            "5120/5120 [==============================] - 4s 838us/step - loss: 0.6679 - val_loss: 1.1035\n",
            "Epoch 17/30\n",
            "5120/5120 [==============================] - 4s 832us/step - loss: 0.6331 - val_loss: 1.0902\n",
            "Epoch 18/30\n",
            "5120/5120 [==============================] - 4s 825us/step - loss: 0.6035 - val_loss: 1.0817\n",
            "Epoch 19/30\n",
            "5120/5120 [==============================] - 4s 827us/step - loss: 0.5722 - val_loss: 1.0754\n",
            "Epoch 20/30\n",
            "5120/5120 [==============================] - 4s 826us/step - loss: 0.5452 - val_loss: 1.0686\n",
            "Epoch 21/30\n",
            "5120/5120 [==============================] - 4s 824us/step - loss: 0.5159 - val_loss: 1.0598\n",
            "Epoch 22/30\n",
            "5120/5120 [==============================] - 4s 836us/step - loss: 0.4884 - val_loss: 1.0552\n",
            "Epoch 23/30\n",
            "5120/5120 [==============================] - 4s 826us/step - loss: 0.4626 - val_loss: 1.0492\n",
            "Epoch 24/30\n",
            "5120/5120 [==============================] - 4s 828us/step - loss: 0.4377 - val_loss: 1.0442\n",
            "Epoch 25/30\n",
            "5120/5120 [==============================] - 4s 829us/step - loss: 0.4161 - val_loss: 1.0366\n",
            "Epoch 26/30\n",
            "5120/5120 [==============================] - 4s 870us/step - loss: 0.3935 - val_loss: 1.0362\n",
            "Epoch 27/30\n",
            "5120/5120 [==============================] - 4s 863us/step - loss: 0.3743 - val_loss: 1.0315\n",
            "Epoch 28/30\n",
            "5120/5120 [==============================] - 4s 842us/step - loss: 0.3551 - val_loss: 1.0333\n",
            "Epoch 29/30\n",
            "5120/5120 [==============================] - 4s 824us/step - loss: 0.3367 - val_loss: 1.0279\n",
            "Epoch 30/30\n",
            "5120/5120 [==============================] - 4s 826us/step - loss: 0.3185 - val_loss: 1.0286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_12 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_11/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_11/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Sy_WCp31x79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Section 3 Testing\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "** Task 4 (20 marks) **\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   Use the trained model to translate the text from source into target language (10 marks). Use the test/evaluation set (see Section 1) and perform an automatic evaluation with the BLEU metric (10 marks). use the NLTK library to calculate BLEU.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Yt4IP_ocBwIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get the model and compile and load it\n",
        "model= Model(inputs=[encoder_inputs,decoder_inputs],outputs=decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.load_weights('seq2seq_source_target.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXg4Nacd253k",
        "colab_type": "code",
        "outputId": "147731c4-f1cc-44aa-cc59-f30d1012aa22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1258
        }
      },
      "cell_type": "code",
      "source": [
        "# #Your code goes here\n",
        "#for the testing, here I use the decode_sequence function, using the model to get the\n",
        "#encoder and decoder, according to the decoder prediction, get the index of each word and then\n",
        "#using reverse_target_char_index to get the corresponsing word, if the words number greater than\n",
        "#max sequence number ot its \\n then break the loop, then get the decoder sentence.\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "  [decoder_inputs] + decoder_states_inputs,\n",
        "  [decoder_outputs] + decoder_states)\n",
        "\n",
        "# reverse-lookup token index to turn sequences back to characters\n",
        "reverse_input_char_index = dict(\n",
        "  (i, char) for char, i in dic_sourceVol.items())\n",
        "reverse_target_char_index = dict(\n",
        "  (i, char) for char, i in dic_targetVol.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # encode the input sequence to get the internal state vectors.\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "  # generate empty target sequence of length 1 with only the start character\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  target_seq[0, 0, dic_targetVol['\\t']] = 1.\n",
        "  # output sequence loop\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict(\n",
        "      [target_seq] + states_value)\n",
        "    # sample a token and add the corresponding character to the \n",
        "    # decoded sequence\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "    # check for the exit condition: either hitting max length\n",
        "    # or predicting the 'stop' character\n",
        "    if (sampled_char == '\\n' or \n",
        "        len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        "      \n",
        "    # update the target sequence (length 1).\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "    # update states\n",
        "    states_value = [h, c]   \n",
        "  return decoded_sentence\n",
        "\n",
        "for seq_index in range(10):\n",
        "  input_seq = X_test_encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input sentence:', test_input_text[seq_index])\n",
        "  decoded_sentence=nltk.word_tokenize(decoded_sentence)\n",
        "  print('Decoded sentence:', decoded_sentence)\n",
        "  if ('\\t' in test_target_text[seq_index] and '\\n' in test_target_text[seq_index]):\n",
        "    test_target_text[seq_index].remove('\\t')\n",
        "    test_target_text[seq_index].remove('\\n')\n",
        "  reference=[i.lower() for i in test_target_text[seq_index]]\n",
        "  candidate=decoded_sentence\n",
        "  print('Target sentence:', reference)\n",
        "  BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], candidate)\n",
        "  BLEUscore2 = nltk.translate.bleu_score.sentence_bleu([reference], candidate,weights=(0.25,0.25,0.25,0.25))\n",
        "  print('BLEU score: ', BLEUscore)\n",
        "  print('Ngram BLEU score: ', BLEUscore2)\n",
        "\n",
        "  \n",
        "#testing \n",
        "#this is for all testing data, the BLEU score use 4-gram, and caculate the sum of each gram, the average \n",
        "#BLEU score is 0.47963007282241854\n",
        "BLEUscore_testing=0\n",
        "BLEUscore2_testing=0\n",
        "\n",
        "for seq_index in range(len(X_test_encoder_input_data)):\n",
        "  pos_same_sentence=[]\n",
        "  input_seq = X_test_encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  decoded_sentence=nltk.word_tokenize(decoded_sentence)\n",
        "  if ('\\t' in test_target_text[seq_index] and '\\n' in test_target_text[seq_index]):\n",
        "    test_target_text[seq_index].remove('\\t')\n",
        "    test_target_text[seq_index].remove('\\n')\n",
        "  reference=[i.lower() for i in test_target_text[seq_index]]\n",
        "  candidate=decoded_sentence\n",
        "  BLEUscore2_testing =BLEUscore2_testing+nltk.translate.bleu_score.sentence_bleu([reference], candidate,weights=(0.25,0.25,0.25,0.25))\n",
        "aver_BLEUscore2=BLEUscore2_testing/(len(X_test_encoder_input_data))\n",
        "print('Ngram BLEU score: ', aver_BLEUscore2)\n",
        "  "
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ['You', \"'re\", 'nuts', '!']\n",
            "Decoded sentence: ['vous', 'êtes', 'dingue']\n",
            "Target sentence: [\"t'es\", 'givrée', '!']\n",
            "BLEU score:  0\n",
            "Ngram BLEU score:  0\n",
            "-\n",
            "Input sentence: ['Tom', 'snores', '.']\n",
            "Decoded sentence: ['tom', 'a', 'parlé', '.']\n",
            "Target sentence: ['tom', 'ronfle', '.']\n",
            "BLEU score:  0.8408964152537145\n",
            "Ngram BLEU score:  0.8408964152537145\n",
            "-\n",
            "Input sentence: ['They', \"'re\", 'lying', '.']\n",
            "Decoded sentence: ['elles', 'sont', '.']\n",
            "Target sentence: ['elles', 'mentent', '.']\n",
            "BLEU score:  0.9036020036098448\n",
            "Ngram BLEU score:  0.9036020036098448\n",
            "-\n",
            "Input sentence: ['Go', 'get', 'it', '.']\n",
            "Decoded sentence: ['va', 'tout', 'changer']\n",
            "Target sentence: ['va', 'le', 'chercher', '!']\n",
            "BLEU score:  0.5444460596606694\n",
            "Ngram BLEU score:  0.5444460596606694\n",
            "-\n",
            "Input sentence: ['Come', 'at', 'once', '.']\n",
            "Decoded sentence: ['venez', 'ici', '.']\n",
            "Target sentence: ['viens', 'ici', 'immédiatement', '.']\n",
            "BLEU score:  0.6474591278836639\n",
            "Ngram BLEU score:  0.6474591278836639\n",
            "-\n",
            "Input sentence: ['Hang', 'on', '.']\n",
            "Decoded sentence: ['tenez', 'bien', '!']\n",
            "Target sentence: ['tiens', 'bon', '!']\n",
            "BLEU score:  0.7598356856515925\n",
            "Ngram BLEU score:  0.7598356856515925\n",
            "-\n",
            "Input sentence: ['Is', 'this', 'a', 'gag', '?']\n",
            "Decoded sentence: ['est-ce', 'une', 'blague']\n",
            "Target sentence: [\"s'agit-il\", \"d'une\", 'blague', '?']\n",
            "BLEU score:  0.5444460596606694\n",
            "Ngram BLEU score:  0.5444460596606694\n",
            "-\n",
            "Input sentence: ['I', \"'m\", 'sneaky', '.']\n",
            "Decoded sentence: ['je', 'suis', 'sournoise']\n",
            "Target sentence: ['je', 'suis', 'sournois', '.']\n",
            "BLEU score:  0.5444460596606694\n",
            "Ngram BLEU score:  0.5444460596606694\n",
            "-\n",
            "Input sentence: ['Am', 'I', 'stupid', '?']\n",
            "Decoded sentence: ['suis-je', 'pas', '?']\n",
            "Target sentence: ['suis-je', 'idiot', '?']\n",
            "BLEU score:  0.9036020036098448\n",
            "Ngram BLEU score:  0.9036020036098448\n",
            "-\n",
            "Input sentence: ['Hello', 'everyone', '!']\n",
            "Decoded sentence: ['salut', 'tout', 'le']\n",
            "Target sentence: ['salut', 'les', 'gens', '!']\n",
            "BLEU score:  0.5444460596606694\n",
            "Ngram BLEU score:  0.5444460596606694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Ngram BLEU score:  0.47963007282241854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wb4F1-a00Hw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Section 4 Attention\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6XTD-fCC1yUA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Task 5 (40 Marks) **sequence2sequence\n",
        "\n",
        "* Extend the existing Seq2Seq model with an attention mechanism [Discussed in Class]\n",
        "* Create sequence2sequence model with attention (15 marks)\n",
        "* Train the model with the same data from Section 1 (10 marks)\n",
        "* Translate the evaluation using the sequence2sequence attention model (10 marks)\n",
        "* Evaluate the translations made with the sequence2sequence attention model and compare it with the model without attention using BLEU (5 marks)"
      ]
    },
    {
      "metadata": {
        "id": "1laP89jZ7_Y5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for the attention model, the first part is almost same with previous model\n",
        "#the input shape of encoder and decoder are same to previous one, here using the\n",
        "#feed the encoder and decoder to LSTM and then return sequences, get the initial_state with \n",
        "#decoder input data, then feed them into the LSTM as decoder\n",
        "from keras.layers import SimpleRNN\n",
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "encoder_input=Input(shape=(None,len(embeddings_source[0])))\n",
        "decoder_input=Input(shape=(None,num_decoder_tokens))\n",
        "\n",
        "encoder=LSTM(latent_dim, return_sequences=True)(encoder_input)\n",
        "encoder_last = encoder[:,-1,:]\n",
        "\n",
        "decoder=LSTM(latent_dim, return_sequences=True)(decoder_input, initial_state=[encoder_last, encoder_last])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bI1kVhUh9aRe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f3842391-826c-4d66-e766-aecc660a770f"
      },
      "cell_type": "code",
      "source": [
        "#reference: https://github.com/wanasit/katakana/blob/master/notebooks/Attention-based%20Sequence-to-Sequence%20in%20Keras.ipynb\n",
        "#Attention\n",
        "# here I use the coding of reference above, the formula comes from \n",
        "# reference:https://arxiv.org/pdf/1508.04025.pdf\n",
        "from keras.layers import Activation, dot, concatenate\n",
        "# Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
        "# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
        "attention = dot([decoder, encoder], axes=[2, 2])\n",
        "attention = Activation('softmax', name='attention')(attention)\n",
        "print('attention', attention)\n",
        "\n",
        "context = dot([attention, encoder], axes=[2,1])\n",
        "print('context', context)\n",
        "\n",
        "decoder_combined_context = concatenate([context, decoder])\n",
        "print('decoder_combined_context', decoder_combined_context)\n",
        "# Has another weight + tanh layer as described in equation (5) of the paper\n",
        "output = TimeDistributed(Dense(latent_dim, activation=\"tanh\"))(decoder_combined_context)\n",
        "output = TimeDistributed(Dense(num_decoder_tokens, activation=\"softmax\"))(output)\n",
        "print('output', output)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention Tensor(\"attention_5/truediv:0\", shape=(?, ?, ?), dtype=float32)\n",
            "context Tensor(\"dot_12/MatMul:0\", shape=(?, ?, 256), dtype=float32)\n",
            "decoder_combined_context Tensor(\"concatenate_6/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
            "output Tensor(\"time_distributed_12/Reshape_1:0\", shape=(?, ?, 3948), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ybBCc2Ld-CEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "1cedb202-8cc5-4946-bcab-9933e3d0dd8c"
      },
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_attention = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
        "model_attention.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model_attention.summary()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           (None, None, 3948)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_25 (InputLayer)           (None, None, 50)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_16 (LSTM)                  (None, None, 256)    4305920     input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_15 (LSTM)                  (None, None, 256)    314368      input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_11 (Dot)                    (None, None, None)   0           lstm_16[0][0]                    \n",
            "                                                                 lstm_15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, None, None)   0           dot_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_12 (Dot)                    (None, None, 256)    0           attention[0][0]                  \n",
            "                                                                 lstm_15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, None, 512)    0           dot_12[0][0]                     \n",
            "                                                                 lstm_16[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_11 (TimeDistri (None, None, 256)    131328      concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_12 (TimeDistri (None, None, 3948)   1014636     time_distributed_11[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 5,766,252\n",
            "Trainable params: 5,766,252\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X5vbPUaWBlgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1108
        },
        "outputId": "d5410029-5cdb-481c-e10b-4792300ab49d"
      },
      "cell_type": "code",
      "source": [
        "#training the model  \n",
        "model_attention.fit([X_train_encoder_input_data, X_train_decoder_input_data], y_train_decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "model_attention.save('seq2seq_source_target_attention.h5')"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5120 samples, validate on 1280 samples\n",
            "Epoch 1/30\n",
            "5120/5120 [==============================] - 9s 2ms/step - loss: 2.1478 - val_loss: 1.9651\n",
            "Epoch 2/30\n",
            "5120/5120 [==============================] - 4s 838us/step - loss: 1.8440 - val_loss: 1.9113\n",
            "Epoch 3/30\n",
            "5120/5120 [==============================] - 4s 836us/step - loss: 1.7656 - val_loss: 1.8416\n",
            "Epoch 4/30\n",
            "5120/5120 [==============================] - 4s 842us/step - loss: 1.6673 - val_loss: 1.7750\n",
            "Epoch 5/30\n",
            "5120/5120 [==============================] - 4s 845us/step - loss: 1.6014 - val_loss: 1.7291\n",
            "Epoch 6/30\n",
            "5120/5120 [==============================] - 4s 846us/step - loss: 1.5490 - val_loss: 1.6874\n",
            "Epoch 7/30\n",
            "5120/5120 [==============================] - 5s 887us/step - loss: 1.5001 - val_loss: 1.6494\n",
            "Epoch 8/30\n",
            "5120/5120 [==============================] - 5s 882us/step - loss: 1.4556 - val_loss: 1.6110\n",
            "Epoch 9/30\n",
            "5120/5120 [==============================] - 4s 841us/step - loss: 1.4073 - val_loss: 1.5798\n",
            "Epoch 10/30\n",
            "5120/5120 [==============================] - 4s 842us/step - loss: 1.3649 - val_loss: 1.5410\n",
            "Epoch 11/30\n",
            "5120/5120 [==============================] - 4s 847us/step - loss: 1.3244 - val_loss: 1.5013\n",
            "Epoch 12/30\n",
            "5120/5120 [==============================] - 4s 841us/step - loss: 1.2792 - val_loss: 1.4599\n",
            "Epoch 13/30\n",
            "5120/5120 [==============================] - 4s 844us/step - loss: 1.2251 - val_loss: 1.4181\n",
            "Epoch 14/30\n",
            "5120/5120 [==============================] - 4s 840us/step - loss: 1.1711 - val_loss: 1.3726\n",
            "Epoch 15/30\n",
            "5120/5120 [==============================] - 4s 842us/step - loss: 1.1198 - val_loss: 1.3446\n",
            "Epoch 16/30\n",
            "5120/5120 [==============================] - 5s 890us/step - loss: 1.0665 - val_loss: 1.3135\n",
            "Epoch 17/30\n",
            "5120/5120 [==============================] - 5s 892us/step - loss: 1.0136 - val_loss: 1.2864\n",
            "Epoch 18/30\n",
            "5120/5120 [==============================] - 4s 847us/step - loss: 0.9632 - val_loss: 1.2530\n",
            "Epoch 19/30\n",
            "5120/5120 [==============================] - 4s 841us/step - loss: 0.9138 - val_loss: 1.2442\n",
            "Epoch 20/30\n",
            "5120/5120 [==============================] - 4s 841us/step - loss: 0.8658 - val_loss: 1.2089\n",
            "Epoch 21/30\n",
            "5120/5120 [==============================] - 4s 844us/step - loss: 0.8196 - val_loss: 1.1969\n",
            "Epoch 22/30\n",
            "5120/5120 [==============================] - 4s 839us/step - loss: 0.7715 - val_loss: 1.1824\n",
            "Epoch 23/30\n",
            "5120/5120 [==============================] - 4s 846us/step - loss: 0.7266 - val_loss: 1.1712\n",
            "Epoch 24/30\n",
            "5120/5120 [==============================] - 4s 844us/step - loss: 0.6835 - val_loss: 1.1495\n",
            "Epoch 25/30\n",
            "5120/5120 [==============================] - 4s 846us/step - loss: 0.6431 - val_loss: 1.1385\n",
            "Epoch 26/30\n",
            "5120/5120 [==============================] - 4s 841us/step - loss: 0.6015 - val_loss: 1.1316\n",
            "Epoch 27/30\n",
            "5120/5120 [==============================] - 4s 840us/step - loss: 0.5635 - val_loss: 1.1222\n",
            "Epoch 28/30\n",
            "5120/5120 [==============================] - 4s 843us/step - loss: 0.5258 - val_loss: 1.1079\n",
            "Epoch 29/30\n",
            "5120/5120 [==============================] - 4s 838us/step - loss: 0.4925 - val_loss: 1.1024\n",
            "Epoch 30/30\n",
            "5120/5120 [==============================] - 4s 844us/step - loss: 0.4568 - val_loss: 1.0946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_16 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'strided_slice_4:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'strided_slice_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHGZzyaqSuer",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get the model and compile and load it\n",
        "model_attention= Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
        "model_attention.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model_attention.load_weights('seq2seq_source_target_attention.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4s4keOjsTh3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0ec9b7e6-fa51-4fd4-9b4e-038dfd6398b0"
      },
      "cell_type": "code",
      "source": [
        "# #Your code goes here\n",
        "#for the testing, here I use model.predict directly, the input data\n",
        "#are testing encoder data and testing decoder data, same to previous one\n",
        "#here I can get the index of each output, and then using recerse_target_char_index to reverse\n",
        "#the index to word, then tokenize it and compare the sentence with original sentence, \n",
        "#then using nltk.bleu to get score, the final score is 0.5336994507415952, it improved\n",
        "# nearly 6%.\n",
        "def attention_testing():\n",
        "  decoded_sentences=[]\n",
        "  predictions=model_attention.predict([X_test_encoder_input_data, X_test_decoder_input_data])\n",
        "  for i in range(predictions.shape[0]):\n",
        "    decoded_sentence=''\n",
        "    for j in range(predictions.shape[1]):\n",
        "        output_tokens=predictions[i][j]\n",
        "        sampled_token_index2 = np.argmax(output_tokens)\n",
        "        sampled_char2 = reverse_target_char_index[sampled_token_index2]  \n",
        "        if (sampled_char2 == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "          break\n",
        "        decoded_sentence += ' '+sampled_char2\n",
        "        \n",
        "    decoded_sentences.append(decoded_sentence)\n",
        "  return decoded_sentences\n",
        "decoded_sentences=attention_testing()\n",
        "BLEUscore2_testing2=0\n",
        "for i in range(len(decoded_sentences)):\n",
        "    reference=[j.lower() for j in test_target_text[i]]\n",
        "    decoded_sentence2=nltk.word_tokenize(decoded_sentences[i])\n",
        "    candidate=decoded_sentence2\n",
        "    BLEUscore2_testing2 =BLEUscore2_testing2+nltk.translate.bleu_score.sentence_bleu([reference], candidate,weights=(0.25,0.25,0.25,0.25))\n",
        "aver_BLEUscore2=BLEUscore2_testing2/(len(X_test_encoder_input_data))\n",
        "print('Ngram BLEU score: ', aver_BLEUscore2)  "
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Ngram BLEU score:  0.5336994507415952\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}